{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Here are all the parameters you need to set before running\n",
        "\n",
        "1. Set params\n",
        "2. Click \"Run All\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Migrate Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mode list(no need to change it)\n",
        "_FROM_BIBTEX_TO_DT = 0      # Migrate from the bibtex, which manually exported from Zotero\n",
        "_FROM_ZOTERO_TO_DT = 1      # Migrate from Zotero to Devonthink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the migration mode\n",
        "MODE = _FROM_BIBTEX_TO_DT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Result Save Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# All the result will be save in this folder, including DEVONthink database\n",
        "RESULT_FOLDER = \"/Users/tftuser/Desktop/Migrate/Exported Items_Sandy Carpenter\"\n",
        "# The path to store the clean pdfs from Zotero. These pdfs are renamed, with extension. Recommended also in the RESULT_FOLDER\n",
        "RESULT_FOLDER_DOCUMENTS = \"/Users/tftuser/Desktop/Migrate/Exported Items_Sandy Carpenter/files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zotero Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if migrating from Zotero\n",
        "if MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # The path to Zotero Sqlite database\n",
        "    ZOTERO_DATABASE_PATH = \"/Users/tftuser/Desktop/New Zotero/zotero_new.sqlite\"           # \"/Users/tftuser/Zotero/zotero.sqlite\"\n",
        "    # The path to Zotero storage path (we need it to locate the attachments)\n",
        "    ZOTERO_STORAGE_PATH = \"/Users/tftuser/Desktop/New Zotero/storage\"\n",
        "    # The library name that we want to export from Zotero. (LIBRARY_NAME has higher priority than COLLECTION_NAME, COLLECTION_NAME will work only when LIBRARY_NAME is None)\n",
        "    LIBRARY_NAME = \"Carolynne Snowden\"       # \"Elyn Garret\"        # \"Effie Burrus\"\n",
        "    # Collection name that we want to export. If set None, export all collections\n",
        "    COLLECTION_NAME = None      # \"Effie Burrus--Michigan and Cleveland\"\n",
        "    # Zotero metadata tsv\n",
        "    METADATA_TSV_NAME = \"metadata.tsv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DEVONthink Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If we migrate from the bibtex, which manually exported from Zotero\n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    # The path to the folder of unzipped exported files, \"UNZIP_BASE_PATH + Exported Items_Sandy Carpenter.bib\" should be able to locate the bib file\n",
        "    UNZIP_BASE_PATH = \"/Users/tftuser/Desktop/Migrate/Exported Items_Sandy Carpenter\"          # \"/Users/tftuser/Desktop/Migrate/Exported Items_Vashti McKensie\"\n",
        "    # The bibtex file name\n",
        "    BIB_NAME = \"Exported Items_Sandy Carpenter.bib\"     # \"Exported Items_Vashti McKensie.bib\"\n",
        "\n",
        "# The DEVONthink database name\n",
        "DEVONTHINK_DATABASE_NAME = \"Sandy Carpenter.dtBase2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Metadata from Zotero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use local sqlite to get data\n",
        "\n",
        "- need to append filepath information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n%pip install sqlite3\\n%pip install pandas\\n%pip install Pyarrow        # pandas required dependency\\n%pip install tabulate\\n'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "%pip install sqlite3\n",
        "%pip install pandas\n",
        "%pip install Pyarrow        # pandas required dependency\n",
        "%pip install tabulate\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the metadata of the attachments\n",
        "def get_SQL_METADATA(libraryID, collection_key):\n",
        "    \"\"\"\n",
        "    Collection might has sub collections\n",
        "    \"\"\"\n",
        "\n",
        "    sql = f\"\"\"\n",
        "        -- Get the attachment and its information\n",
        "        SELECT\n",
        "            items.key,      -- key of the attachment, also the folder name\n",
        "            fields.fieldName,            -- field name\n",
        "            itemDataValues.value AS fieldValue,         -- field value\n",
        "            tag_info.tag,          -- tags info, duplicated for every key\n",
        "            CASE\n",
        "                WHEN fields.fieldName='title' THEN items.key || '/' || SUBSTR(itemAttachments.path, LENGTH('storage:') + 1)\n",
        "                ELSE NULL\n",
        "            END AS file        -- the file path\n",
        "        FROM\n",
        "            items\n",
        "                -- Filter only items as attachment, which can be pdf, html, etc.\n",
        "                INNER JOIN itemAttachments ON items.itemID = itemAttachments.itemID\n",
        "                -- Get Metadata, using parentItemID because the info is saved on the parent not on the attachment\n",
        "                LEFT JOIN itemData ON itemAttachments.parentItemID=itemData.itemID\n",
        "                -- Get field name\n",
        "                LEFT JOIN fields ON fields.fieldID=itemData.fieldID\n",
        "                -- Get field\n",
        "                LEFT JOIN itemDataValues ON itemDataValues.valueID=itemData.valueID\n",
        "                -- tag\n",
        "                LEFT JOIN (SELECT itemTags.itemID, GROUP_CONCAT(DISTINCT tags.name) AS tag\n",
        "                           FROM itemTags LEFT JOIN tags ON itemTags.tagID=tags.tagID\n",
        "                           GROUP BY itemTags.itemID\n",
        "                          ) AS tag_info ON itemAttachments.parentItemID=tag_info.itemID\n",
        "        \"\"\"\n",
        "    \n",
        "    # If needs to filter based on the library\n",
        "    if (libraryID is not None) and (libraryID != \"\"):\n",
        "        sql += \"\\n\" + f\"\"\"\n",
        "            WHERE \n",
        "                items.libraryID={libraryID}\n",
        "\n",
        "        \"\"\"\n",
        "    # If needs to filter based on the collection\n",
        "    elif (collection_key is not None) and (collection_key != \"\"):\n",
        "        sql += \"\\n\" + f\"\"\"\n",
        "            WHERE\n",
        "                  -- Make sure the attachments from a specific collections\n",
        "                  itemAttachments.parentItemID IN (\n",
        "                      SELECT collectionItems.itemID\n",
        "                      FROM collectionItems\n",
        "                               LEFT JOIN collections ON collections.collectionID = collectionItems.collectionID\n",
        "                      WHERE collections.key=\\'{collection_key}\\'\n",
        "                  )\n",
        "            \"\"\"\n",
        "\n",
        "    return sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Get metadata from the database\n",
        "def get_metadata(zotero_database_path, library_name=None, collection_name=None, save_path=None):\n",
        "    \"\"\"\n",
        "    :param save_path: save the metadata as tsv\n",
        "    \"\"\"\n",
        "    # Get libraryID based on the library name\n",
        "    def _get_libraryID(library_name, zotero_database_path):\n",
        "        SQL_LIBRARY = f\"\"\"\n",
        "            SELECT libraries.libraryID\n",
        "            FROM libraries\n",
        "                LEFT JOIN groups on libraries.libraryID = groups.libraryID\n",
        "            WHERE groups.name='{library_name}'\n",
        "        \"\"\"\n",
        "\n",
        "        conn = sqlite3.connect(zotero_database_path)        \n",
        "        try:\n",
        "            df = pd.read_sql_query(SQL_LIBRARY, conn)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        finally:\n",
        "            conn.close()\n",
        "\n",
        "        # It should be only one result\n",
        "        if len(df[\"libraryID\"]) > 0:\n",
        "            return df[\"libraryID\"][0]\n",
        "        else:\n",
        "            # print(f\"No Library Named: {library_name} !!!\")\n",
        "            return None\n",
        "    \n",
        "    # Get collection key based on the collection name\n",
        "    def _get_collection_key(collection_name, zotero_database_path):\n",
        "        SQL_COLLECTION = f\"\"\"\n",
        "            SELECT collections.key\n",
        "            FROM collections\n",
        "            WHERE collections.collectionName=\\'{collection_name}\\'\n",
        "        \"\"\"\n",
        "\n",
        "        conn = sqlite3.connect(zotero_database_path)        \n",
        "        try:\n",
        "            df = pd.read_sql_query(SQL_COLLECTION, conn)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        finally:\n",
        "            conn.close()\n",
        "\n",
        "        # It should be only one result\n",
        "        if len(df[\"key\"]) > 0:\n",
        "            return df[\"key\"][0]\n",
        "        else:\n",
        "            # print(f\"No Collection Named: {collection_name} !!!\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    libraryID = None\n",
        "    collection_key = None\n",
        "    # Get libraryID\n",
        "    if library_name is not None and library_name != \"\":\n",
        "        libraryID = _get_libraryID(library_name=library_name, zotero_database_path=zotero_database_path)\n",
        "        if libraryID is None or libraryID == \"\":\n",
        "            print(f\"No Library Named: {library_name} !!!\")\n",
        "            sys.exit(0)\n",
        "    # Get the collection key\n",
        "    elif collection_name is not None and collection_name != \"\":\n",
        "        collection_key = _get_collection_key(collection_name=collection_name, zotero_database_path=zotero_database_path)\n",
        "        if collection_key is None or collection_key == \"\":\n",
        "            print(f\"No Collection Named: {collection_name} !!!\")\n",
        "            sys.exit(0)\n",
        "    else:\n",
        "        collection_key = None\n",
        "\n",
        "    # Get metadata from Zotero\n",
        "    conn = sqlite3.connect(zotero_database_path)\n",
        "    try:\n",
        "        # Query\n",
        "        sql = get_SQL_METADATA(libraryID=libraryID, collection_key=collection_key)\n",
        "        df = pd.read_sql_query(sql, conn)\n",
        "        # Save the metadata content\n",
        "        if save_path is not None:\n",
        "            df.to_csv(save_path, sep=\"\\t\", index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pivot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate       # print(tabulate(res, headers='keys', tablefmt='pretty'))\n",
        "\n",
        "\n",
        "# Pivot table\n",
        "def pivot(ori_df, save_path=None):\n",
        "    # Function to get the first non-NaN value\n",
        "    def _first_not_nan(series):\n",
        "        return series.dropna().iloc[0] if not series.dropna().empty else np.nan\n",
        "    \n",
        "    # Pivot the table\n",
        "    res_part1 = ori_df.copy()\n",
        "    res_part1 = res_part1.pivot_table(index=\"key\", columns=\"fieldName\", values=\"fieldValue\", aggfunc=\"first\")   # res = res.pivot(index='key', columns='fieldName', values='fieldValue')\n",
        "\n",
        "    # Deal with the remaining columns\n",
        "    res_part2 = ori_df.copy()\n",
        "    remain_cols = list(set(res_part2.columns) - set([\"key\", \"fieldName\", \"fieldValue\"]))     # key field is still inside\n",
        "    agg_dict = {i_col: _first_not_nan for i_col in remain_cols}     # the other fields except [\"keys\", \"fieldName\", \"fieldValue\"]\n",
        "    res_part2 = res_part2.groupby('key').agg(agg_dict).reset_index()\n",
        "\n",
        "    # Join both parts\n",
        "    res = pd.merge(res_part1, res_part2, on='key', how='left')\n",
        "\n",
        "    # Save the result\n",
        "    if save_path is not None:\n",
        "        res.to_csv(save_path, sep=\"\\t\", index=False)\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fix the tsv\n",
        "\n",
        "- drop key\n",
        "- rename abstractnote into abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fix_csv(df, save_path=None):\n",
        "    # Rename key into foldername\n",
        "    if \"key\" in df.columns:\n",
        "        df = df.rename(columns={\"key\": \"foldername\"})    # df.drop(\"key\", axis=1)\n",
        "    \n",
        "    # Rename abstractnote into abstract\n",
        "    if \"abstractNote\" in df.columns:\n",
        "        df = df.rename(columns={\"abstractNote\": \"abstract\"})\n",
        "\n",
        "    # Rename tag into keywords\n",
        "    if \"tag\" in df.columns:\n",
        "        df = df.rename(columns={\"tag\": \"keywords\"})\n",
        "\n",
        "    # Extra Citation into a seperate column\n",
        "    if \"extra\" in df.columns and \"citationKey\" not in df.columns:\n",
        "        df['citationkey'] = df['extra'].str.extract('Citation Key: ([^\\n]*)')\n",
        "\n",
        "\n",
        "    # Save the result\n",
        "    if save_path is not None:\n",
        "        df.to_csv(save_path, sep=\"\\t\", index=False)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export\n",
        "\n",
        "- The whole thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_zotero(zotero_database_path, library_name, collection_name, save_path=None):\n",
        "    res = get_metadata(zotero_database_path, library_name, collection_name, save_path=None)\n",
        "    res = pivot(res, save_path=None)\n",
        "    res = fix_csv(res, save_path=None)\n",
        "\n",
        "    # Save the path\n",
        "    if save_path is not None:\n",
        "        res.to_csv(save_path, sep=\"\\t\", index=False) \n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXoU9y0Mwe5g"
      },
      "source": [
        "# Convert bib into csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ2ozEsix-Lh",
        "outputId": "6301e797-6352-4606-92ab-ad0d11df9d94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n%pip install --upgrade pip setuptools\\n%pip install bibtexparser\\n%pip install pybtex\\n%pip install --upgrade pybtex\\n'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "%pip install --upgrade pip setuptools\n",
        "%pip install bibtexparser\n",
        "%pip install pybtex\n",
        "%pip install --upgrade pybtex\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solution 3:\n",
        "\n",
        "- Use both pybtex and bibtexparser\n",
        "    - Use pybtex to change the ENTRYTYPE to msci\n",
        "    - Use bibtextparser to read and write the data\n",
        "    - Recover the ENTRYTYPE in the csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pybtex.database import parse_file, Entry\n",
        "from pybtex.database.output.bibtex import Writer\n",
        "import bibtexparser\n",
        "from bibtexparser.bparser import BibTexParser\n",
        "from bibtexparser.customization import convert_to_unicode\n",
        "import csv\n",
        "\n",
        "# Change all ENTRYTYPE into misc\n",
        "def _change_entrytype(bib_in_path, bib_out_path): \n",
        "    \"\"\"\n",
        "    # Change all ENTRYTYPE into @misc\n",
        "    # IT DOESN\"T WORK OUT AFTER TO_STRING()??? SO WEIRD!!!\n",
        "    bib_data = parse_file(bib_in_path, bib_format=\"bibtex\")\n",
        "    for key, entry in bib_data.entries.items():\n",
        "        entry.type = 'misc'\n",
        "    bib_data = bib_data.to_string(bib_format=\"bibtex\")\n",
        "\n",
        "    writer = Writer()\n",
        "    with open(bib_out_path, 'w', encoding='utf-8') as output_file:\n",
        "        writer.write_stream(bib_data, output_file)\n",
        "    \"\"\"\n",
        "\n",
        "    with open(bib_in_path, 'r', encoding='utf-8') as input_file:\n",
        "        bib_data = input_file.read()\n",
        "\n",
        "    # First to_string, then regex\n",
        "    entrytype_pattern = re.compile(r'@\\w+{')\n",
        "    bib_data = re.sub(entrytype_pattern, '@misc{', bib_data)\n",
        "\n",
        "    # To output to the file\n",
        "    with open(bib_out_path, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(bib_data)\n",
        "\n",
        "# Function to load and parse the BibTeX file\n",
        "def _load_bibtex(bib_file_path):\n",
        "    with open(bib_file_path, encoding='utf-8') as bibtex_file:\n",
        "        parser = BibTexParser(common_strings=True)\n",
        "        parser.customization = convert_to_unicode\n",
        "        bib_database = bibtexparser.load(bibtex_file, parser=parser)\n",
        "    return bib_database\n",
        "\n",
        "# Function to find all unique field names in the BibTeX database\n",
        "def _get_bibtex_fieldnames(bib_database):\n",
        "    fieldnames = []\n",
        "    for entry in bib_database.entries:\n",
        "        for key in entry.keys():\n",
        "            if key not in fieldnames:\n",
        "                fieldnames.append(key)\n",
        "    return fieldnames\n",
        "\n",
        "# Function to write the BibTeX database to a CSV file\n",
        "def _write_bibtex2csv(bib_database, csv_file_path, fieldnames):\n",
        "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\\t')\n",
        "        writer.writeheader()\n",
        "        for entry in bib_database.entries:\n",
        "            writer.writerow({field: entry.get(field, '') for field in fieldnames})\n",
        "\n",
        "# Main conversion function\n",
        "def convert_bibtex_to_csv(bib_file_path, csv_file_path):\n",
        "    _tmp_bib_path = \"./tmp_bib.bib\"\n",
        "\n",
        "    _change_entrytype(bib_file_path, _tmp_bib_path)\n",
        "    bib_database = _load_bibtex(_tmp_bib_path)\n",
        "    \n",
        "    fieldnames = _get_bibtex_fieldnames(bib_database)\n",
        "    _write_bibtex2csv(bib_database, csv_file_path, fieldnames)\n",
        "\n",
        "    # Delete the temporary bib file\n",
        "    os.remove(_tmp_bib_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWuTLdaPxPsi"
      },
      "source": [
        "# Fix CSV and files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2eMbIpCI-6-"
      },
      "source": [
        "## File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "just one file per folder (watch for ;) \n",
        "delete any extra attachments\n",
        "    - what would extra attchment look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EWonAA1XxaB0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def one_file(df, col=\"file\"):\n",
        "    def _split_files(file_column_value):\n",
        "        # Split using a regular expression that looks ahead for \"files/\"\n",
        "        parts = re.split(r'(?=files/)', file_column_value)\n",
        "        remove_blank = [part for part in parts if part.strip()]\n",
        "        remove_semicolon = [part.strip(\";\") for part in remove_blank]\n",
        "        return remove_semicolon\n",
        "\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "\n",
        "    # Set as str. Otherwise the blank value might be seen as float NaN\n",
        "    df[col] = df[col].astype(str)\n",
        "\n",
        "    # Split files\n",
        "    df[col] = df[col].apply(_split_files)\n",
        "    # Seperate records into different rows\n",
        "    df = df.explode(col)\n",
        "    # Reset the index (Those splitted rows share the same index now.)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove the records without a file\n",
        "\n",
        "- Not really necessary\n",
        "- The blank files won't bother importing into Deonthink\n",
        "- Yeah, but it doesn't hurt to make the program more robust. Devonthink might cause an error one day. Who know."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_no_file(df, col=\"file\"):\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "\n",
        "    df = df[df[col].notna() & (df[col] != '')]\n",
        "    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "add extension\n",
        "\n",
        "- Some files have no extension, append to it\n",
        "- Pls make sure use both of them. Because you need to guarantee the file path and file field are consistent.\n",
        "- Very special case\n",
        "    + Some files without ext might have blankspaces in the end, but no blankspace in bib file path \n",
        "    + That's why we need to strip before append the extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "__TXT = ['txt', 'doc', 'docx', 'pdf', 'rtf', 'html', 'htm', 'xml', 'md', 'epub', 'mobi', 'azw']\n",
        "__PIC = ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'tiff', 'svg', 'webp']\n",
        "__WEB = ['html', 'htm', 'css', 'js']\n",
        "__AUDIO = ['mp3', 'wav', 'aac', 'flac', 'alac', 'ogg', 'm4a']\n",
        "__VIDEO = ['mp4', 'avi', 'mov', 'wmv', 'flv', 'mkv', 'webm']\n",
        "__ARCHIVE = ['zip', 'rar', '7z', 'tar', 'gz', 'bz2', 'xz']\n",
        "__EXE = ['exe', 'msi', 'bin', 'sh', 'bat']\n",
        "__PPT = ['ppt', 'pptx', 'odp']\n",
        "__EXCEL = ['xls', 'xlsx', 'ods', 'csv']\n",
        "\n",
        "_EXT_LIST = __TXT + __PIC + __WEB + __AUDIO + __VIDEO + __ARCHIVE + __EXE + __PPT + __EXCEL\n",
        "\n",
        "\n",
        "# Add extension to records in the csv\n",
        "def add_extension(df, file_in_dir, file_out_dir, col=\"file\", ext=\"pdf\", output=True, keep_original_file=True):\n",
        "    \"\"\"\n",
        "        The function won't infect Zotero databse\n",
        "    \n",
        "        :param file_in_dir: file_in_dir + file column from df should make a full path of the pdfs\n",
        "        :param file_out_dir: the copy of the file. But clean renamed. file_out_dir + file column after cleaning should make a full path of the pdfs\n",
        "    \"\"\"\n",
        "\n",
        "    # Count how many files have been modified\n",
        "    cnt_change = 0\n",
        "\n",
        "    # Function to append '.pdf' if necessary\n",
        "    def _append_ext(old_filepath):\n",
        "        if old_filepath == \"\":\n",
        "            new_filepath = old_filepath\n",
        "        else:\n",
        "            file_extension = old_filepath.split('.')[-1].lower()\n",
        "            # Need to add the extension\n",
        "            if file_extension not in _EXT_LIST:\n",
        "                # The weird case that some files without ext might have blankspaces in the end, but no blankspace in bib file path \n",
        "                new_filepath = old_filepath.strip()\n",
        "                new_filepath = new_filepath + \".\" + ext\n",
        "\n",
        "                # cnt_change + 1\n",
        "                cnt_change += 1\n",
        "            # The old filename is already valid\n",
        "            else:\n",
        "                new_filepath = old_filepath\n",
        "\n",
        "        # Rename filename in the file manager (OS)\n",
        "        # Form the full path\n",
        "        old_full_path = os.path.join(file_in_dir, old_filepath)\n",
        "        new_full_path = os.path.join(file_out_dir, new_filepath)\n",
        "        # Move\n",
        "        if old_full_path != new_full_path:\n",
        "            # Create a destination fodler is not exist\n",
        "            os.makedirs(os.path.dirname(new_full_path), exist_ok=True)\n",
        "            if keep_original_file:\n",
        "                shutil.copyfile(old_full_path, new_full_path)\n",
        "            else:\n",
        "                os.rename(old_full_path, new_full_path)\n",
        "\n",
        "        return new_filepath  \n",
        "\n",
        "    # Skip when no col\n",
        "    if col not in df.columns:\n",
        "        if output:\n",
        "            print(f\"No Column {col} in the Sheet.\")\n",
        "        return df\n",
        "    \n",
        "    # Execute\n",
        "    df[col] = df[col].apply(_append_ext)\n",
        "\n",
        "    # Print\n",
        "    if output:\n",
        "        print(f\"File Extension Added: {cnt_change} files\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change file name to the new convention\n",
        "\n",
        "- Old file name has ', ;, \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "_PUNCTUATION_MAP = {\n",
        "    \"\\'\": \"\",\n",
        "    \"\\\"\": \"\",\n",
        "    \"$\": \"\",\n",
        "    \";\": \" \",\n",
        "    \",\": \" \",\n",
        "    \"...\": \".\"\n",
        "}\n",
        "\n",
        "\n",
        "# replace the file field\n",
        "def replace_punctuation_csv(df, file_in_dir, file_out_dir, col=\"file\", keep_original_file=False):\n",
        "    \"\"\"\n",
        "        The function won't infect Zotero databse\n",
        "    \n",
        "        :param file_in_dir: file_in_dir + file column from df should make a full path of the pdfs\n",
        "        :param file_out_dir: the copy of the file. But clean renamed. file_out_dir + file column after cleaning should make a full path of the pdfs\n",
        "    \"\"\"\n",
        "\n",
        "    # Function to apply the punctuation map to a file path\n",
        "    def _clean_file_path(old_file_path, punctuation_map):\n",
        "        new_file_path = old_file_path\n",
        "        for key, value in punctuation_map.items():\n",
        "            new_file_path = new_file_path.replace(key, value)\n",
        "        \n",
        "        # Rename filename in the file manager (OS)\n",
        "        old_full_path = os.path.join(file_in_dir, old_file_path)\n",
        "        new_full_path = os.path.join(file_out_dir, new_file_path)\n",
        "        # Might change\n",
        "        if old_full_path != new_full_path:\n",
        "            # Create a destination fodler is not exist\n",
        "            os.makedirs(os.path.dirname(new_full_path), exist_ok=True)\n",
        "            # Move\n",
        "            if keep_original_file:\n",
        "                shutil.copyfile(old_full_path, new_full_path)\n",
        "            else:\n",
        "                os.rename(old_full_path, new_full_path)\n",
        "\n",
        "        return new_file_path\n",
        "\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "    \n",
        "    # Apply the function to the 'file_path' column of the dataframe\n",
        "    df[col] = df[col].apply(_clean_file_path, punctuation_map=_PUNCTUATION_MAP)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ipyFk-0VHL"
      },
      "source": [
        "Make sure file column is the at the first column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def move_column(df, col=\"file\", pos=0):\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "\n",
        "    # Remove the column from the DataFrame\n",
        "    column_to_move_data = df.pop(col)\n",
        "\n",
        "    # Insert the column at the desired position\n",
        "    df.insert(pos, col, column_to_move_data)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## @ field \n",
        "\n",
        "- After converting bib into csv, @ field is changed into ID, we need to convert it back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rename_ID(df, old=\"ID\", new=\"@\"):\n",
        "    if old not in df.columns:\n",
        "        return df\n",
        "    \n",
        "    df = df.rename(columns={old: new})\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG_kKqRSJEPD"
      },
      "source": [
        "## Keywords: replace / with -\n",
        "\n",
        "Example: (African American-Caribbean-African, Los Angeles-Calif) is one keyword.  \n",
        "\n",
        "Keywords should be separated by commas.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "8gY4v597JG8L"
      },
      "outputs": [],
      "source": [
        "def replace_seperator(df, col=\"keywords\", old=\"/\", new=\"-\"):\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "\n",
        "    df[col] = df[col].str.replace(old, new)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZARCAwJHQq"
      },
      "source": [
        "## Title and Abstract: replace smart quotes with ASCII"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "w6P7I3fEJKOV"
      },
      "outputs": [],
      "source": [
        "def replace_quotes(df, cols=[\"title\", \"abstract\"]):\n",
        "    QUOTE_MAP = {\n",
        "        '“': '\"',\n",
        "        '”': '\"',\n",
        "        '‘': \"'\",\n",
        "        '’': \"'\"\n",
        "    }\n",
        "\n",
        "    for i_col in cols:\n",
        "        if i_col in df.columns:\n",
        "            df[i_col] = df[i_col].replace(QUOTE_MAP, regex=True)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEOW8gjgUrJk"
      },
      "source": [
        "## Fix the comma format\n",
        "\n",
        "- Devonthink has weird problems when reading continuous blank spaces in the row end\n",
        "- So it needs one comma at the end of the row (Devonthink is weird)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "zgANd54jU47c"
      },
      "outputs": [],
      "source": [
        "# NOT GOOD FOR THE CSV, BUT KEEP APPLE SCRIPT NICE\n",
        "# Fill enough commas for every row\n",
        "def add_comma(csv_path_in, csv_path_out):\n",
        "    with open(csv_path_in, 'r', encoding='utf-8') as infile, open(csv_path_out, 'w+', encoding='utf-8') as outfile:\n",
        "        # Read the header and write it unchanged\n",
        "        header = infile.readline()\n",
        "        outfile.write(header)\n",
        "\n",
        "        # Append a comma to the end of each subsequent line\n",
        "        for line in infile:\n",
        "            # If the line is not empty, append a comma\n",
        "            if line.strip():\n",
        "                outfile.write(line.rstrip('\\n') + ',\\n')\n",
        "            # Write empty lines unchanged\n",
        "            else:\n",
        "                outfile.write(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOT GOOD FOR APPLE SCRIPT, BUT KEEP CSV NICE\n",
        "# Remember to SKIP the last col in apple script\n",
        "def add_extra_column(df):\n",
        "    _USELESS_COL = \"USELESS\"\n",
        "    _USELESS_VALUE = \"USELESS_USELESS\"\n",
        "\n",
        "    df[_USELESS_COL] = _USELESS_VALUE\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix All\n",
        "\n",
        "- Integrate everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xHp7R9O4UbXM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def fix_all(file_in_path, file_out_path, csv_path_in, csv_path_out=None):\n",
        "    \"\"\"\n",
        "    \n",
        "    :param files_in_path: \n",
        "    \"\"\"\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_path_in, sep=\"\\t\")\n",
        "\n",
        "    # Clean the data\n",
        "    # file\n",
        "    df = remove_no_file(df)\n",
        "    df = one_file(df)\n",
        "    df = move_column(df)\n",
        "    df = add_extension(df, file_in_dir=file_in_path, file_out_dir=file_out_path, col=\"file\", ext=\"pdf\", output=True, keep_original_file=True)\n",
        "    df = replace_punctuation_csv(df, file_in_dir=file_out_path, file_out_dir=file_out_path, col=\"file\", keep_original_file=False)   # file_in_path should be assigned file_out_path, because it's already moved by add_extension\n",
        "    # keyword\n",
        "    df = replace_seperator(df)\n",
        "    # abstract and title\n",
        "    df = replace_quotes(df)\n",
        "    # Conver ID field back into @\n",
        "    df = rename_ID(df, old=\"ID\", new=\"@\")\n",
        "    # Fix last column problem\n",
        "    df = add_extra_column(df)\n",
        "\n",
        "    # Write the modified DataFrame back to a new CSV file\n",
        "    if csv_path_out is not None:\n",
        "        df.to_csv(csv_path_out, sep=\"\\t\", index=False)     # Replace the previous \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJzBCZ9paw2C"
      },
      "source": [
        "# Call AppleScript to Import into DEVONthink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "MvwllvY4JRcg"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "def call_apple_script_file(script_path):\n",
        "    result = subprocess.run(['osascript', script_path], capture_output=True, text=True)\n",
        "\n",
        "    # Check the return code (0 is success)\n",
        "    return result.returncode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4xC_SoWvdhR5"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "def call_apple_script(script):\n",
        "    result = subprocess.run(['osascript', \"-e\", script], capture_output=True, text=True)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run the Whole Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export From Zotero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODE == _FROM_ZOTERO_TO_DT:\n",
        "    export_zotero(zotero_database_path=ZOTERO_DATABASE_PATH, library_name=LIBRARY_NAME, collection_name=COLLECTION_NAME, save_path=RESULT_FOLDER+\"/\"+METADATA_TSV_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Bibtex to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    # The Bib path\n",
        "    BIB_PATH = UNZIP_BASE_PATH + \"/\" + BIB_NAME\n",
        "    # The dirty csv path, which is converted from biblatex\n",
        "    CSV_ORI_PATH = UNZIP_BASE_PATH + \"/\" + \"csv_original.tsv\"\n",
        "# \n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # The dirty csv path, which is exported from Zotero\n",
        "    CSV_ORI_PATH = RESULT_FOLDER + \"/\" + METADATA_TSV_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    convert_bibtex_to_csv(bib_file_path=BIB_PATH, csv_file_path=CSV_ORI_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    # The path is \"Unzipped Exported\", in which \"files/XXXXXXX(number)/xxx.pdf\" from file field\n",
        "    ORIGINAL_DOCUMENT_FOLDER_PATH = UNZIP_BASE_PATH\n",
        "    # The clean document path. (if you don't want to keep the original file, then just use the same as ORIGINAL_DOCUMENT_FOLDER_PATH)\n",
        "    CLEAN_DOCUMENT_FOLDER_PATH = UNZIP_BASE_PATH\n",
        "    # The csv path after cleaning\n",
        "    CSV_CLEAN_PATH = UNZIP_BASE_PATH + \"/\" + \"csv_clean.tsv\"\n",
        "# \n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # For Zotero, path is \"zotero/storage\", in which XXXXXXX(key)/xxx.pdf\n",
        "    ORIGINAL_DOCUMENT_FOLDER_PATH = ZOTERO_STORAGE_PATH\n",
        "    # The path to save the clean files, which are copied from Zotero database and renamed.\n",
        "    CLEAN_DOCUMENT_FOLDER_PATH = RESULT_FOLDER + \"/\" + \"files\"\n",
        "    # The csv path after cleaning\n",
        "    CSV_CLEAN_PATH = RESULT_FOLDER + \"/\" + METADATA_TSV_NAME\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Extension Added: 0 files\n"
          ]
        }
      ],
      "source": [
        "# Fix the csv\n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    fix_all(file_in_path=ORIGINAL_DOCUMENT_FOLDER_PATH, file_out_path=CLEAN_DOCUMENT_FOLDER_PATH, csv_path_in=CSV_ORI_PATH, csv_path_out=CSV_CLEAN_PATH)\n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    fix_all(file_in_path=ORIGINAL_DOCUMENT_FOLDER_PATH, file_out_path=CLEAN_DOCUMENT_FOLDER_PATH, csv_path_in=CSV_ORI_PATH, csv_path_out=CSV_CLEAN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Applescript to Migrate to Devonthink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    # The path of the pdfs (The location of the the files folder, not inside)\n",
        "    DOCSPATH = CLEAN_DOCUMENT_FOLDER_PATH\n",
        "    # The path of the CSV local file after cleaning\n",
        "    CSVPATH_L = CSV_CLEAN_PATH\n",
        "    # Database name\n",
        "    DEVONTHINK_DATABASE_NAME = DEVONTHINK_DATABASE_NAME\n",
        "    # The destination of the exported Davonthink.\n",
        "    DB_FOLDER_PATH = UNZIP_BASE_PATH\n",
        "# \n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # The path of the pdfs (The location of the the files folder, not inside). DOCSPATH + file field should make a full path to the document\n",
        "    DOCSPATH = CLEAN_DOCUMENT_FOLDER_PATH\n",
        "    # The path of the CSV local file after cleaning\n",
        "    CSVPATH_L = CSV_CLEAN_PATH\n",
        "    # Database name\n",
        "    DEVONTHINK_DATABASE_NAME = DEVONTHINK_DATABASE_NAME\n",
        "    # The destination of the exported Davonthink.\n",
        "    DB_FOLDER_PATH = RESULT_FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This can dynamically adapt to the columns in the csv\n",
        "def apple_script_import2devonthink(docspath, csvpath_l, devonthink_database_name, db_folder_path):\n",
        "\tscript = f\"\"\"\n",
        "\t\t-- The path of the pdfs\n",
        "\t\tset DOCSPATH to \"{docspath}\"\n",
        "\n",
        "\t\t-- The path of the CSV local file after cleaning\n",
        "\t\tset CSVPATH_L to \"{csvpath_l}\"\n",
        "\n",
        "\t\t-- Database name\n",
        "\t\tset DBNAME to \"{devonthink_database_name}\"\n",
        "\t\t-- The destination of the exported Davonthink.\n",
        "\t\tset DB_FOLDER_PATH to \"{db_folder_path}\"\n",
        "\t\tset DBPATH to DB_FOLDER_PATH & \"/\" & DBNAME\n",
        "\n",
        "\n",
        "\t\ttell application id \"DNtp\"\n",
        "\n",
        "\t\t\t-- Create the new database\n",
        "\t\t\ttry\n",
        "\t\t\t\t-- Create the new database\n",
        "\t\t\t\tset newDb to create database POSIX file DBPATH as string\n",
        "\t\t\t\t-- set current database\n",
        "\t\t\t\tset curDb to current database\n",
        "\t\t\t\tlog \"Database created successfully at: \" & DBPATH\n",
        "\t\t\ton error errMsg number errorNumber\n",
        "\t\t\t\tdisplay dialog \"Failed to create database: \" & errMsg\n",
        "\t\t\t\treturn\n",
        "\t\t\tend try\n",
        "\n",
        "\n",
        "\t\t\t-- Import the cleaned csv\n",
        "\t\t\tset csv_id to import CSVPATH_L to current group\n",
        "\t\t\t-- Read the csv file from DevonThink (Because it's easier to handle within DavonThink.)\n",
        "\t\t\tset _csv_loc to location of csv_id\n",
        "\t\t\tset _csv_name to name of csv_id\n",
        "\t\t\tset _csv_loc_D to _csv_loc & _csv_name\n",
        "\t\t\tset csvFile to get record at (_csv_loc_D) in curDb\n",
        "\t\t\t-- Get the header names for the metadata names\n",
        "\t\t\tset csvHeaders to (columns of csvFile)\n",
        "\t\t\t-- Get the contents of the cells in the file\n",
        "\t\t\tset csvContents to (cells of csvFile)\n",
        "\n",
        "\n",
        "\t\t\t-- Traverse all the records in the CSV\n",
        "\t\t\trepeat with csvItem in csvContents\n",
        "\n",
        "\t\t\t\t-- Import the file, assuming the file path is in the first column\n",
        "\t\t\t\tset pdfFile to import (DOCSPATH & \"/\" & (item 1 of csvItem)) to current group\n",
        "\n",
        "\t\t\t\t-- Add the custom metadata dynamically based on the number of columns\n",
        "\t\t\t\t-- Item 1 is the file path, so we traverse from item 2\n",
        "\t\t        -- Minus one, because we add one useless column in the end\n",
        "\t\t\t\trepeat with i from 2 to ((count of csvHeaders) - 1)\n",
        "\t\t\t\t\tset mdKey to (item i of csvHeaders) as string\n",
        "\t\t\t\t\tset mdValue to (item i of csvItem)\n",
        "\t\t\t\t\tadd custom meta data mdValue for mdKey to pdfFile\n",
        "\t\t\t\tend repeat\n",
        "\n",
        "\t\t\tend repeat\n",
        "\n",
        "\t\t\t-- Delete that csv\n",
        "\t\t\tdelete record csv_id\t\t-- set theRecord to search \"csv_clean\"\n",
        "\n",
        "\t\tend tell\n",
        "\t\"\"\"\n",
        "\n",
        "\treturn script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "script = apple_script_import2devonthink(docspath=DOCSPATH, csvpath_l=CSVPATH_L, devonthink_database_name=DEVONTHINK_DATABASE_NAME, db_folder_path=DB_FOLDER_PATH)\n",
        "res = call_apple_script(script=script)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some tools for you to validate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check the number of files\n",
        "\n",
        "- May contain some multiple files in the file column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    DOCUMENT_FOLDER_PATH = UNZIP_BASE_PATH + \"/\" + \"files\"\n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    DOCUMENT_FOLDER_PATH = CLEAN_DOCUMENT_FOLDER_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_folder(folder_path):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(\"Error: The specified path is not a directory.\")\n",
        "        return\n",
        "\n",
        "    file_count = 0\n",
        "    file_name = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        # Remove hidden items (Assume we won't set pdf or html as hidden files.)\n",
        "        files = [s for s in files if not s.startswith('.')]\n",
        "\n",
        "\n",
        "        file_count += len(files)\n",
        "        if len(files) > 0:\n",
        "            file_name += files\n",
        "\n",
        "\n",
        "    return file_count, file_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 8 files in the folder '/Users/tftuser/Desktop/Migrate/Exported Items_Sandy Carpenter/files' and its subfolders.\n",
            "\n",
            "Here are the file lists\n",
            "carpenterBurningIllusionCruel1983.pdf\n",
            "carpenterDAWNEXCLUSIVEInterview1987.pdf\n",
            "carpenterEddieMurphyPOSITIVE1986.pdf\n",
            "carpenterEntertainmentJumpinWhoopi1986.pdf\n",
            "carpenterEyeWitnessNews1989.pdf\n",
            "carpenterFilmFestivalJourney1984.pdf\n",
            "carpenterMidnightJazzLovers1986.pdf\n",
            "carpenterMurphyLendsGolden1987.pdf\n"
          ]
        }
      ],
      "source": [
        "num_files, name_files = count_files_in_folder(folder_path=DOCUMENT_FOLDER_PATH)\n",
        "print(f\"There are {num_files} files in the folder '{DOCUMENT_FOLDER_PATH}' and its subfolders.\")\n",
        "print()\n",
        "\n",
        "print(\"Here are the file lists\")\n",
        "name_files.sort(key=lambda x: x.lower())\n",
        "for i_file in name_files:\n",
        "    print(i_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some useful excel functions\n",
        "\n",
        "1. Get the file name from the file path\n",
        "\n",
        "```excel\n",
        "=MID(A51, FIND(\"@\", SUBSTITUTE(A51, \"/\", \"@\", LEN(A51)-LEN(SUBSTITUTE(A51, \"/\", \"\"))))+1, LEN(A51))\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
