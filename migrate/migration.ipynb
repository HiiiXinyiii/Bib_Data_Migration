{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Here are all the parameters you need to set before running\n",
        "\n",
        "1. Set params\n",
        "2. Click \"Run All\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Related Libraries Param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "INSTALL_RELATED_LIBRARY = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Migrate Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mode list(no need to change it)\n",
        "_FROM_BIBTEX_TO_DT = 0      # Migrate from the bibtex, which manually exported from Zotero\n",
        "_FROM_ZOTERO_TO_DT = 1      # Migrate from Zotero to Devonthink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the migration mode\n",
        "MODE = _FROM_ZOTERO_TO_DT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Source Params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From Zotero Params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where To Get the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if migrating from Zotero\n",
        "if MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # The path to Zotero Sqlite database\n",
        "    ZOTERO_DATABASE_PATH = \"/Users/tftuser/Zotero/zotero.sqlite\"\n",
        "    # The path to Zotero storage path (we need it to locate the attachments)\n",
        "    ZOTERO_STORAGE_PATH = \"/Users/tftuser/Zotero/storage\"\n",
        "    # The library name that we want to export from Zotero. (LIBRARY_NAME has higher priority than COLLECTION_NAME, COLLECTION_NAME will work only when LIBRARY_NAME is None)\n",
        "    LIBRARY_NAME = \"Pamala Haynes\"       # \"Elyn Garret\"        # \"Effie Burrus\"\n",
        "    # Collection name that we want to export. If set None, export all collections\n",
        "    COLLECTION_NAME = None      # \"Effie Burrus--Michigan and Cleveland\"\n",
        "    # Zotero metadata tsv\n",
        "    METADATA_TSV_NAME = \"metadata.tsv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where To Save the Result\n",
        "- The exported documents and DEVONthink database will be both saved in this folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if migrating from Zotero\n",
        "if MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # All the result will be save in this folder, including DEVONthink database\n",
        "    RESULT_FOLDER = \"/Users/tftuser/Desktop/Migrate/Exported Items_Pamala Haynes\"\n",
        "    # The path to store the clean pdfs from Zotero. These pdfs are renamed, with extension. Recommended also in the RESULT_FOLDER\n",
        "    RESULT_FOLDER_DOCUMENTS = \"/Users/tftuser/Desktop/Migrate/Exported Items_Pamala Haynes/files\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From Exported Files Params\n",
        "\n",
        "- We exported documents with bib from Zotero manually using plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If we migrate from the bibtex, which manually exported from Zotero\n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    # The path to the folder of unzipped exported files, \"UNZIP_BASE_PATH + Exported Items_Sandy Carpenter.bib\" should be able to locate the bib file\n",
        "    UNZIP_BASE_PATH = \"/Users/tftuser/Desktop/Migrate/Exported Items_Black Women Film Critics 316\"\n",
        "    # The bibtex file name\n",
        "    BIB_NAME = \"Exported Items_Black Women Film Critics 316.bib\"     # \"Exported Items_Vashti McKensie.bib\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DEVONthink Params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The DEVONthink database name\n",
        "DEVONTHINK_DATABASE_NAME = \"Pamala Haynes.dtBase2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Metadata from Zotero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use local sqlite to get data\n",
        "\n",
        "- need to append filepath information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jupyter might show error red wave line, just ignore it. It's fine.\n",
        "if INSTALL_RELATED_LIBRARY:\n",
        "    %pip install pandas\n",
        "    %pip install Pyarrow        # pandas required dependency\n",
        "    %pip install tabulate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the metadata of the attachments\n",
        "def get_SQL_METADATA(libraryID, collection_key):\n",
        "    \"\"\"\n",
        "    Collection might has sub collections\n",
        "    \"\"\"\n",
        "\n",
        "    sql = f\"\"\"\n",
        "        -- Get the attachment and its information\n",
        "        SELECT\n",
        "            items.key,      -- key of the attachment, also the folder name\n",
        "            fields.fieldName,            -- field name\n",
        "            itemDataValues.value AS fieldValue,         -- field value\n",
        "            tag_info.tag,          -- tags info, duplicated for every key\n",
        "            CASE\n",
        "                WHEN fields.fieldName='title' THEN items.key || '/' || SUBSTR(itemAttachments.path, LENGTH('storage:') + 1)\n",
        "                ELSE NULL\n",
        "            END AS file        -- the file path\n",
        "        FROM\n",
        "            items\n",
        "                -- Filter only items as attachment, which can be pdf, html, etc.\n",
        "                INNER JOIN itemAttachments ON items.itemID=itemAttachments.itemID\n",
        "                -- Get Metadata, using parentItemID because the info is saved on the parent not on the attachment\n",
        "                LEFT JOIN itemData ON itemAttachments.parentItemID=itemData.itemID\n",
        "                -- Get field name\n",
        "                LEFT JOIN fields ON fields.fieldID=itemData.fieldID\n",
        "                -- Get field\n",
        "                LEFT JOIN itemDataValues ON itemDataValues.valueID=itemData.valueID\n",
        "                -- tag\n",
        "                LEFT JOIN (SELECT itemTags.itemID, GROUP_CONCAT(DISTINCT tags.name) AS tag\n",
        "                           FROM itemTags LEFT JOIN tags ON itemTags.tagID=tags.tagID\n",
        "                           GROUP BY itemTags.itemID\n",
        "                          ) AS tag_info ON itemAttachments.parentItemID=tag_info.itemID\n",
        "                -- Exclude Deleted Items (Needs deletedItems.itemID=NULL in WHERE)\n",
        "                LEFT JOIN deletedItems ON items.itemID=deletedItems.itemID\n",
        "        \"\"\"\n",
        "    \n",
        "    # If needs to filter based on the library\n",
        "    if (libraryID is not None) and (libraryID != \"\"):\n",
        "        sql += \"\\n\" + f\"\"\"\n",
        "            WHERE \n",
        "                items.libraryID={libraryID}\n",
        "                -- Exclude Deleted Items \n",
        "                AND deletedItems.itemID IS NULL\n",
        "\n",
        "        \"\"\"\n",
        "    # If needs to filter based on the collection\n",
        "    elif (collection_key is not None) and (collection_key != \"\"):\n",
        "        sql += \"\\n\" + f\"\"\"\n",
        "            WHERE\n",
        "                  -- Make sure the attachments from a specific collections\n",
        "                  itemAttachments.parentItemID IN (\n",
        "                      SELECT collectionItems.itemID\n",
        "                      FROM collectionItems\n",
        "                               LEFT JOIN collections ON collections.collectionID = collectionItems.collectionID\n",
        "                      WHERE collections.key=\\'{collection_key}\\'\n",
        "                  )\n",
        "                  -- Exclude Deleted Items \n",
        "                  AND deletedItems.itemID IS NULL\n",
        "            \"\"\"\n",
        "\n",
        "    return sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Get metadata from the database\n",
        "def get_metadata(zotero_database_path, library_name=None, collection_name=None, save_path=None):\n",
        "    \"\"\"\n",
        "    :param save_path: save the metadata as tsv\n",
        "    \"\"\"\n",
        "    # Get libraryID based on the library name\n",
        "    def _get_libraryID(library_name, zotero_database_path):\n",
        "        SQL_LIBRARY = f\"\"\"\n",
        "            SELECT libraries.libraryID\n",
        "            FROM libraries\n",
        "                LEFT JOIN groups on libraries.libraryID = groups.libraryID\n",
        "            WHERE groups.name='{library_name}'\n",
        "        \"\"\"\n",
        "\n",
        "        conn = sqlite3.connect(zotero_database_path)        \n",
        "        try:\n",
        "            df = pd.read_sql_query(SQL_LIBRARY, conn)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        finally:\n",
        "            conn.close()\n",
        "\n",
        "        # It should be only one result\n",
        "        if len(df[\"libraryID\"]) > 0:\n",
        "            return df[\"libraryID\"][0]\n",
        "        else:\n",
        "            # print(f\"No Library Named: {library_name} !!!\")\n",
        "            return None\n",
        "    \n",
        "    # Get collection key based on the collection name\n",
        "    def _get_collection_key(collection_name, zotero_database_path):\n",
        "        SQL_COLLECTION = f\"\"\"\n",
        "            SELECT collections.key\n",
        "            FROM collections\n",
        "            WHERE collections.collectionName=\\'{collection_name}\\'\n",
        "        \"\"\"\n",
        "\n",
        "        conn = sqlite3.connect(zotero_database_path)        \n",
        "        try:\n",
        "            df = pd.read_sql_query(SQL_COLLECTION, conn)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "        finally:\n",
        "            conn.close()\n",
        "\n",
        "        # It should be only one result\n",
        "        if len(df[\"key\"]) > 0:\n",
        "            return df[\"key\"][0]\n",
        "        else:\n",
        "            # print(f\"No Collection Named: {collection_name} !!!\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    libraryID = None\n",
        "    collection_key = None\n",
        "    # Get libraryID\n",
        "    if library_name is not None and library_name != \"\":\n",
        "        libraryID = _get_libraryID(library_name=library_name, zotero_database_path=zotero_database_path)\n",
        "        if libraryID is None or libraryID == \"\":\n",
        "            print(f\"No Library Named: {library_name} !!!\")\n",
        "            sys.exit(0)\n",
        "    # Get the collection key\n",
        "    elif collection_name is not None and collection_name != \"\":\n",
        "        collection_key = _get_collection_key(collection_name=collection_name, zotero_database_path=zotero_database_path)\n",
        "        if collection_key is None or collection_key == \"\":\n",
        "            print(f\"No Collection Named: {collection_name} !!!\")\n",
        "            sys.exit(0)\n",
        "    else:\n",
        "        collection_key = None\n",
        "\n",
        "    # Get metadata from Zotero\n",
        "    conn = sqlite3.connect(zotero_database_path)\n",
        "    try:\n",
        "        # Query\n",
        "        sql = get_SQL_METADATA(libraryID=libraryID, collection_key=collection_key)\n",
        "        df = pd.read_sql_query(sql, conn)\n",
        "        # Save the metadata content\n",
        "        if save_path is not None:\n",
        "            df.to_csv(save_path, sep=\"\\t\", index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    finally:\n",
        "        conn.close()\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pivot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tabulate import tabulate       # print(tabulate(res, headers='keys', tablefmt='pretty'))\n",
        "\n",
        "\n",
        "# Pivot table\n",
        "def pivot(ori_df, save_path=None):\n",
        "    # Function to get the first non-NaN value\n",
        "    def _first_not_nan(series):\n",
        "        return series.dropna().iloc[0] if not series.dropna().empty else np.nan\n",
        "    \n",
        "    # Pivot the table\n",
        "    res_part1 = ori_df.copy()\n",
        "    res_part1 = res_part1.pivot_table(index=\"key\", columns=\"fieldName\", values=\"fieldValue\", aggfunc=\"first\")   # res = res.pivot(index='key', columns='fieldName', values='fieldValue')\n",
        "\n",
        "    # Deal with the remaining columns\n",
        "    res_part2 = ori_df.copy()\n",
        "    remain_cols = list(set(res_part2.columns) - set([\"key\", \"fieldName\", \"fieldValue\"]))     # key field is still inside\n",
        "    agg_dict = {i_col: _first_not_nan for i_col in remain_cols}     # the other fields except [\"keys\", \"fieldName\", \"fieldValue\"]\n",
        "    res_part2 = res_part2.groupby('key').agg(agg_dict).reset_index()\n",
        "\n",
        "    # Join both parts\n",
        "    res = pd.merge(res_part1, res_part2, on='key', how='left')\n",
        "\n",
        "    # Save the result\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)      # Create a destination fodler is not exist\n",
        "        res.to_csv(save_path, sep=\"\\t\", index=False)\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fix the tsv\n",
        "\n",
        "- drop key\n",
        "- rename abstractnote into abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fix_csv(df, save_path=None):\n",
        "    # Rename key into foldername\n",
        "    if \"key\" in df.columns:\n",
        "        df = df.rename(columns={\"key\": \"foldername\"})    # df.drop(\"key\", axis=1)\n",
        "    \n",
        "    # Rename abstractnote into abstract\n",
        "    if \"abstractNote\" in df.columns:\n",
        "        df = df.rename(columns={\"abstractNote\": \"abstract\"})\n",
        "\n",
        "    # Rename tag into keywords\n",
        "    if \"tag\" in df.columns:\n",
        "        df = df.rename(columns={\"tag\": \"keywords\"})\n",
        "\n",
        "    # Extra Citation into a seperate column\n",
        "    if \"extra\" in df.columns and \"citationKey\" not in df.columns:\n",
        "        df['citationkey'] = df['extra'].str.extract('Citation Key: ([^\\n]*)')\n",
        "\n",
        "\n",
        "    # Save the result\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)      # Create a destination fodler is not exist\n",
        "        df.to_csv(save_path, sep=\"\\t\", index=False)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export\n",
        "\n",
        "- The whole thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def export_zotero(zotero_database_path, library_name, collection_name, save_path=None):\n",
        "    res = get_metadata(zotero_database_path, library_name, collection_name, save_path=None)\n",
        "    res = pivot(res, save_path=None)\n",
        "    res = fix_csv(res, save_path=None)\n",
        "\n",
        "    # Save the path\n",
        "    if save_path is not None:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)      # Create a destination fodler is not exist\n",
        "        res.to_csv(save_path, sep=\"\\t\", index=False) \n",
        "\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXoU9y0Mwe5g"
      },
      "source": [
        "# Convert bib into csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ2ozEsix-Lh",
        "outputId": "6301e797-6352-4606-92ab-ad0d11df9d94"
      },
      "outputs": [],
      "source": [
        "# Jupyter might show error red wave line, just ignore it. It's fine.\n",
        "if INSTALL_RELATED_LIBRARY:\n",
        "    %pip install --upgrade pip setuptools\n",
        "    %pip install bibtexparser\n",
        "    %pip install pybtex\n",
        "    %pip install --upgrade pybtex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solution 3:\n",
        "\n",
        "- Use both pybtex and bibtexparser\n",
        "    - Use pybtex to change the ENTRYTYPE to msci\n",
        "    - Use bibtextparser to read and write the data\n",
        "    - Recover the ENTRYTYPE in the csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pybtex.database import parse_file, Entry\n",
        "from pybtex.database.output.bibtex import Writer\n",
        "import bibtexparser\n",
        "from bibtexparser.bparser import BibTexParser\n",
        "from bibtexparser.customization import convert_to_unicode\n",
        "import csv\n",
        "\n",
        "# Change all ENTRYTYPE into misc\n",
        "def _change_entrytype(bib_in_path, bib_out_path): \n",
        "    \"\"\"\n",
        "    # Change all ENTRYTYPE into @misc\n",
        "    # IT DOESN\"T WORK OUT AFTER TO_STRING()??? SO WEIRD!!!\n",
        "    bib_data = parse_file(bib_in_path, bib_format=\"bibtex\")\n",
        "    for key, entry in bib_data.entries.items():\n",
        "        entry.type = 'misc'\n",
        "    bib_data = bib_data.to_string(bib_format=\"bibtex\")\n",
        "\n",
        "    writer = Writer()\n",
        "    with open(bib_out_path, 'w', encoding='utf-8') as output_file:\n",
        "        writer.write_stream(bib_data, output_file)\n",
        "    \"\"\"\n",
        "\n",
        "    with open(bib_in_path, 'r', encoding='utf-8') as input_file:\n",
        "        bib_data = input_file.read()\n",
        "\n",
        "    # First to_string, then regex\n",
        "    entrytype_pattern = re.compile(r'@\\w+{')\n",
        "    bib_data = re.sub(entrytype_pattern, '@misc{', bib_data)\n",
        "\n",
        "    # To output to the file\n",
        "    with open(bib_out_path, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(bib_data)\n",
        "\n",
        "# Function to load and parse the BibTeX file\n",
        "def _load_bibtex(bib_file_path):\n",
        "    with open(bib_file_path, encoding='utf-8') as bibtex_file:\n",
        "        parser = BibTexParser(common_strings=True)\n",
        "        parser.customization = convert_to_unicode\n",
        "        bib_database = bibtexparser.load(bibtex_file, parser=parser)\n",
        "    return bib_database\n",
        "\n",
        "# Function to find all unique field names in the BibTeX database\n",
        "def _get_bibtex_fieldnames(bib_database):\n",
        "    fieldnames = []\n",
        "    for entry in bib_database.entries:\n",
        "        for key in entry.keys():\n",
        "            if key not in fieldnames:\n",
        "                fieldnames.append(key)\n",
        "    return fieldnames\n",
        "\n",
        "# Function to write the BibTeX database to a CSV file\n",
        "def _write_bibtex2csv(bib_database, csv_file_path, fieldnames):\n",
        "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter='\\t')\n",
        "        writer.writeheader()\n",
        "        for entry in bib_database.entries:\n",
        "            writer.writerow({field: entry.get(field, '') for field in fieldnames})\n",
        "\n",
        "# Main conversion function\n",
        "def convert_bibtex_to_csv(bib_file_path, csv_file_path):\n",
        "    _tmp_bib_path = \"./tmp_bib.bib\"\n",
        "\n",
        "    _change_entrytype(bib_file_path, _tmp_bib_path)\n",
        "    bib_database = _load_bibtex(_tmp_bib_path)\n",
        "    \n",
        "    fieldnames = _get_bibtex_fieldnames(bib_database)\n",
        "    _write_bibtex2csv(bib_database, csv_file_path, fieldnames)\n",
        "\n",
        "    # Delete the temporary bib file\n",
        "    os.remove(_tmp_bib_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWuTLdaPxPsi"
      },
      "source": [
        "# Fix CSV and files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2eMbIpCI-6-"
      },
      "source": [
        "## File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "just one file per folder (watch for ;) \n",
        "delete any extra attachments\n",
        "    - what would extra attchment look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "EWonAA1XxaB0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def one_file(df, col=\"file\"):\n",
        "    def _split_files(file_column_value):\n",
        "        # Split using a regular expression that looks ahead for \"files/\"\n",
        "        parts = re.split(r'(?=files/)', file_column_value)\n",
        "        remove_blank = [part for part in parts if part.strip()]\n",
        "        remove_semicolon = [part.strip(\";\") for part in remove_blank]\n",
        "        return remove_semicolon\n",
        "\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "\n",
        "    # Set as str. Otherwise the blank value might be seen as float NaN\n",
        "    df[col] = df[col].astype(str)\n",
        "\n",
        "    # Split files\n",
        "    df[col] = df[col].apply(_split_files)\n",
        "    # Seperate records into different rows\n",
        "    df = df.explode(col)\n",
        "    # Reset the index (Those splitted rows share the same index now.)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove the records without a file\n",
        "\n",
        "- Not really necessary\n",
        "- The blank files won't bother importing into Deonthink\n",
        "- Yeah, but it doesn't hurt to make the program more robust. Devonthink might cause an error one day. Who know."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_no_file(df, col=\"file\"):\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "\n",
        "    df = df[df[col].notna() & (df[col] != '')]\n",
        "    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "add extension\n",
        "\n",
        "- Some files have no extension, append to it\n",
        "- Pls make sure use both of them. Because you need to guarantee the file path and file field are consistent.\n",
        "- Very special case\n",
        "    + Some files without ext might have blankspaces in the end, but no blankspace in bib file path \n",
        "    + That's why we need to strip before append the extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "__TXT = ['txt', 'doc', 'docx', 'pdf', 'rtf', 'html', 'htm', 'xml', 'md', 'epub', 'mobi', 'azw']\n",
        "__PIC = ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'tiff', 'svg', 'webp']\n",
        "__WEB = ['html', 'htm', 'css', 'js']\n",
        "__AUDIO = ['mp3', 'wav', 'aac', 'flac', 'alac', 'ogg', 'm4a']\n",
        "__VIDEO = ['mp4', 'avi', 'mov', 'wmv', 'flv', 'mkv', 'webm']\n",
        "__ARCHIVE = ['zip', 'rar', '7z', 'tar', 'gz', 'bz2', 'xz']\n",
        "__EXE = ['exe', 'msi', 'bin', 'sh', 'bat']\n",
        "__PPT = ['ppt', 'pptx', 'odp']\n",
        "__EXCEL = ['xls', 'xlsx', 'ods', 'csv']\n",
        "\n",
        "_EXT_LIST = __TXT + __PIC + __WEB + __AUDIO + __VIDEO + __ARCHIVE + __EXE + __PPT + __EXCEL\n",
        "\n",
        "\n",
        "# Add extension to records in the csv\n",
        "def add_extension(df, file_in_dir, file_out_dir, col=\"file\", ext=\"pdf\", output=True, keep_original_file=True):\n",
        "    \"\"\"\n",
        "        The function won't infect Zotero databse\n",
        "    \n",
        "        :param file_in_dir: file_in_dir + file column from df should make a full path of the pdfs\n",
        "        :param file_out_dir: the copy of the file. But clean renamed. file_out_dir + file column after cleaning should make a full path of the pdfs\n",
        "    \"\"\"\n",
        "\n",
        "    # Count how many files have been modified\n",
        "    cnt_change = 0\n",
        "\n",
        "    # Function to append '.pdf' if necessary\n",
        "    def _append_ext(old_filepath):\n",
        "        if old_filepath == \"\":\n",
        "            new_filepath = old_filepath\n",
        "        else:\n",
        "            file_extension = old_filepath.split('.')[-1].lower()\n",
        "            # Need to add the extension\n",
        "            if file_extension not in _EXT_LIST:\n",
        "                # The weird case that some files without ext might have blankspaces in the end, but no blankspace in bib file path \n",
        "                new_filepath = old_filepath.strip()\n",
        "                new_filepath = new_filepath + \".\" + ext\n",
        "\n",
        "                # cnt_change + 1\n",
        "                cnt_change += 1\n",
        "            # The old filename is already valid\n",
        "            else:\n",
        "                new_filepath = old_filepath\n",
        "\n",
        "        # Rename filename in the file manager (OS)\n",
        "        # Form the full path\n",
        "        old_full_path = os.path.join(file_in_dir, old_filepath)\n",
        "        new_full_path = os.path.join(file_out_dir, new_filepath)\n",
        "        # Move\n",
        "        if old_full_path != new_full_path:\n",
        "            # Create a destination fodler is not exist\n",
        "            os.makedirs(os.path.dirname(new_full_path), exist_ok=True)\n",
        "            if keep_original_file:\n",
        "                shutil.copyfile(old_full_path, new_full_path)\n",
        "            else:\n",
        "                os.rename(old_full_path, new_full_path)\n",
        "\n",
        "        return new_filepath  \n",
        "\n",
        "    # Skip when no col\n",
        "    if col not in df.columns:\n",
        "        if output:\n",
        "            print(f\"No Column {col} in the Sheet.\")\n",
        "        return df\n",
        "    \n",
        "    # Execute\n",
        "    df[col] = df[col].apply(_append_ext)\n",
        "\n",
        "    # Print\n",
        "    if output:\n",
        "        print(f\"File Extension Added: {cnt_change} files\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change file name to the new convention\n",
        "\n",
        "- Old file name has ', ;, \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "_PUNCTUATION_MAP = {\n",
        "    \"\\'\": \"\",\n",
        "    \"\\\"\": \"\",\n",
        "    \"$\": \"\",\n",
        "    \";\": \" \",\n",
        "    \",\": \" \",\n",
        "    \"...\": \".\"\n",
        "}\n",
        "\n",
        "\n",
        "# replace the file field\n",
        "def replace_punctuation_csv(df, file_in_dir, file_out_dir, col=\"file\", keep_original_file=False):\n",
        "    \"\"\"\n",
        "        The function won't infect Zotero databse\n",
        "    \n",
        "        :param file_in_dir: file_in_dir + file column from df should make a full path of the pdfs\n",
        "        :param file_out_dir: the copy of the file. But clean renamed. file_out_dir + file column after cleaning should make a full path of the pdfs\n",
        "    \"\"\"\n",
        "\n",
        "    # Function to apply the punctuation map to a file path\n",
        "    def _clean_file_path(old_file_path, punctuation_map):\n",
        "        new_file_path = old_file_path\n",
        "        for key, value in punctuation_map.items():\n",
        "            new_file_path = new_file_path.replace(key, value)\n",
        "        \n",
        "        # Rename filename in the file manager (OS)\n",
        "        old_full_path = os.path.join(file_in_dir, old_file_path)\n",
        "        new_full_path = os.path.join(file_out_dir, new_file_path)\n",
        "        # Might change\n",
        "        if old_full_path != new_full_path:\n",
        "            # Create a destination fodler is not exist\n",
        "            os.makedirs(os.path.dirname(new_full_path), exist_ok=True)\n",
        "            # Move\n",
        "            if keep_original_file:\n",
        "                shutil.copyfile(old_full_path, new_full_path)\n",
        "            else:\n",
        "                os.rename(old_full_path, new_full_path)\n",
        "\n",
        "        return new_file_path\n",
        "\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "    \n",
        "    # Apply the function to the 'file_path' column of the dataframe\n",
        "    df[col] = df[col].apply(_clean_file_path, punctuation_map=_PUNCTUATION_MAP)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ipyFk-0VHL"
      },
      "source": [
        "Make sure file column is the at the first column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def move_column(df, col=\"file\", pos=0):\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "\n",
        "    # Remove the column from the DataFrame\n",
        "    column_to_move_data = df.pop(col)\n",
        "\n",
        "    # Insert the column at the desired position\n",
        "    df.insert(pos, col, column_to_move_data)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## @ field \n",
        "\n",
        "- After converting bib into csv, @ field is changed into ID, we need to convert it back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rename_ID(df, old=\"ID\", new=\"@\"):\n",
        "    if old not in df.columns:\n",
        "        return df\n",
        "    \n",
        "    df = df.rename(columns={old: new})\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## date field\n",
        "\n",
        "- When migrating from Zotero, the date field is like \"1971-08-17 Aug 17, 1971\". We need to keep just \"1971-08-17\"\n",
        "- When migrating from Exported File, the date is perfectly \"1971-08-17\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keep_one_date(df, col=\"date\"):\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "    \n",
        "    df[col] = df[col].str.strip()\n",
        "    df[col] = df[col].str.split(\" \").str[0]\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG_kKqRSJEPD"
      },
      "source": [
        "## Keywords: replace / with -\n",
        "\n",
        "Example: (African American-Caribbean-African, Los Angeles-Calif) is one keyword.  \n",
        "\n",
        "Keywords should be separated by commas.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "8gY4v597JG8L"
      },
      "outputs": [],
      "source": [
        "def replace_seperator(df, col=\"keywords\", old=\"/\", new=\"-\"):\n",
        "    if col not in df.columns:\n",
        "        return df\n",
        "    \n",
        "    df[col] = df[col].str.replace(old, new)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZARCAwJHQq"
      },
      "source": [
        "## Title and Abstract: replace smart quotes with ASCII"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "w6P7I3fEJKOV"
      },
      "outputs": [],
      "source": [
        "def replace_quotes(df, cols=[\"title\", \"abstract\"]):\n",
        "    QUOTE_MAP = {\n",
        "        '“': '\"',\n",
        "        '”': '\"',\n",
        "        '‘': \"'\",\n",
        "        '’': \"'\"\n",
        "    }\n",
        "\n",
        "    for i_col in cols:\n",
        "        if i_col in df.columns:\n",
        "            df[i_col] = df[i_col].replace(QUOTE_MAP, regex=True)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEOW8gjgUrJk"
      },
      "source": [
        "## Fix the comma format\n",
        "\n",
        "- Devonthink has weird problems when reading continuous blank spaces in the row end\n",
        "- So it needs one comma at the end of the row (Devonthink is weird)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "zgANd54jU47c"
      },
      "outputs": [],
      "source": [
        "# NOT GOOD FOR THE CSV, BUT KEEP APPLE SCRIPT NICE\n",
        "# Fill enough commas for every row\n",
        "def add_comma(csv_path_in, csv_path_out):\n",
        "    with open(csv_path_in, 'r', encoding='utf-8') as infile, open(csv_path_out, 'w+', encoding='utf-8') as outfile:\n",
        "        # Read the header and write it unchanged\n",
        "        header = infile.readline()\n",
        "        outfile.write(header)\n",
        "\n",
        "        # Append a comma to the end of each subsequent line\n",
        "        for line in infile:\n",
        "            # If the line is not empty, append a comma\n",
        "            if line.strip():\n",
        "                outfile.write(line.rstrip('\\n') + ',\\n')\n",
        "            # Write empty lines unchanged\n",
        "            else:\n",
        "                outfile.write(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOT GOOD FOR APPLE SCRIPT, BUT KEEP CSV NICE\n",
        "# Remember to SKIP the last col in apple script\n",
        "def add_extra_column(df):\n",
        "    _USELESS_COL = \"USELESS\"\n",
        "    _USELESS_VALUE = \"USELESS_USELESS\"\n",
        "\n",
        "    df[_USELESS_COL] = _USELESS_VALUE\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix All\n",
        "\n",
        "- Integrate everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xHp7R9O4UbXM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def fix_all(file_in_path, file_out_path, csv_path_in, csv_path_out=None):\n",
        "    \"\"\"\n",
        "    \n",
        "    :param files_in_path: \n",
        "    \"\"\"\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_path_in, sep=\"\\t\")\n",
        "\n",
        "    # Clean the data\n",
        "    # file\n",
        "    df = remove_no_file(df)\n",
        "    df = one_file(df)\n",
        "    df = move_column(df)\n",
        "    df = add_extension(df, file_in_dir=file_in_path, file_out_dir=file_out_path, col=\"file\", ext=\"pdf\", output=True, keep_original_file=True)\n",
        "    df = replace_punctuation_csv(df, file_in_dir=file_out_path, file_out_dir=file_out_path, col=\"file\", keep_original_file=False)   # file_in_path should be assigned file_out_path, because it's already moved by add_extension\n",
        "    # keyword\n",
        "    df = replace_seperator(df)\n",
        "    # abstract and title\n",
        "    df = replace_quotes(df)\n",
        "    # Conver ID field back into @\n",
        "    df = rename_ID(df, old=\"ID\", new=\"@\")\n",
        "    # Keep only one date\n",
        "    df = keep_one_date(df, col=\"date\")\n",
        "    # Fix last column problem\n",
        "    df = add_extra_column(df)\n",
        "\n",
        "    # Write the modified DataFrame back to a new CSV file\n",
        "    if csv_path_out is not None:\n",
        "        df.to_csv(csv_path_out, sep=\"\\t\", index=False)     # Replace the previous \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJzBCZ9paw2C"
      },
      "source": [
        "# Call AppleScript to Import into DEVONthink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "MvwllvY4JRcg"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "def call_apple_script_file(script_path):\n",
        "    result = subprocess.run(['osascript', script_path], capture_output=True, text=True)\n",
        "\n",
        "    # Check the return code (0 is success)\n",
        "    return result.returncode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "4xC_SoWvdhR5"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "def call_apple_script(script):\n",
        "    result = subprocess.run(['osascript', \"-e\", script], capture_output=True, text=True)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run the Whole Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export From Zotero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODE == _FROM_ZOTERO_TO_DT:\n",
        "    export_zotero(zotero_database_path=ZOTERO_DATABASE_PATH, library_name=LIBRARY_NAME, collection_name=COLLECTION_NAME, save_path=RESULT_FOLDER+\"/\"+METADATA_TSV_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert Bibtex to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    # The Bib path\n",
        "    BIB_PATH = UNZIP_BASE_PATH + \"/\" + BIB_NAME\n",
        "    # The dirty csv path, which is converted from biblatex\n",
        "    CSV_ORI_PATH = UNZIP_BASE_PATH + \"/\" + \"csv_original.tsv\"\n",
        "# \n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # The dirty csv path, which is exported from Zotero\n",
        "    CSV_ORI_PATH = RESULT_FOLDER + \"/\" + METADATA_TSV_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    convert_bibtex_to_csv(bib_file_path=BIB_PATH, csv_file_path=CSV_ORI_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fix CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    # The path is \"Unzipped Exported\", in which \"files/XXXXXXX(number)/xxx.pdf\" from file field\n",
        "    ORIGINAL_DOCUMENT_FOLDER_PATH = UNZIP_BASE_PATH\n",
        "    # The clean document path. (if you don't want to keep the original file, then just use the same as ORIGINAL_DOCUMENT_FOLDER_PATH)\n",
        "    CLEAN_DOCUMENT_FOLDER_PATH = UNZIP_BASE_PATH\n",
        "    # The csv path after cleaning\n",
        "    CSV_CLEAN_PATH = UNZIP_BASE_PATH + \"/\" + \"csv_clean.tsv\"\n",
        "# \n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # For Zotero, path is \"zotero/storage\", in which XXXXXXX(key)/xxx.pdf\n",
        "    ORIGINAL_DOCUMENT_FOLDER_PATH = ZOTERO_STORAGE_PATH\n",
        "    # The path to save the clean files, which are copied from Zotero database and renamed.\n",
        "    CLEAN_DOCUMENT_FOLDER_PATH = RESULT_FOLDER + \"/\" + \"files\"\n",
        "    # The csv path after cleaning\n",
        "    CSV_CLEAN_PATH = RESULT_FOLDER + \"/\" + METADATA_TSV_NAME\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File Extension Added: 0 files\n"
          ]
        }
      ],
      "source": [
        "# Fix the csv\n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    fix_all(file_in_path=ORIGINAL_DOCUMENT_FOLDER_PATH, file_out_path=CLEAN_DOCUMENT_FOLDER_PATH, csv_path_in=CSV_ORI_PATH, csv_path_out=CSV_CLEAN_PATH)\n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    fix_all(file_in_path=ORIGINAL_DOCUMENT_FOLDER_PATH, file_out_path=CLEAN_DOCUMENT_FOLDER_PATH, csv_path_in=CSV_ORI_PATH, csv_path_out=CSV_CLEAN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Applescript to Migrate to Devonthink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    # The path of the pdfs (The location of the the files folder, not inside)\n",
        "    DOCSPATH = CLEAN_DOCUMENT_FOLDER_PATH\n",
        "    # The path of the CSV local file after cleaning\n",
        "    CSVPATH_L = CSV_CLEAN_PATH\n",
        "    # Database name\n",
        "    DEVONTHINK_DATABASE_NAME = DEVONTHINK_DATABASE_NAME\n",
        "    # The destination of the exported Davonthink.\n",
        "    DB_FOLDER_PATH = UNZIP_BASE_PATH\n",
        "# \n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    # The path of the pdfs (The location of the the files folder, not inside). DOCSPATH + file field should make a full path to the document\n",
        "    DOCSPATH = CLEAN_DOCUMENT_FOLDER_PATH\n",
        "    # The path of the CSV local file after cleaning\n",
        "    CSVPATH_L = CSV_CLEAN_PATH\n",
        "    # Database name\n",
        "    DEVONTHINK_DATABASE_NAME = DEVONTHINK_DATABASE_NAME\n",
        "    # The destination of the exported Davonthink.\n",
        "    DB_FOLDER_PATH = RESULT_FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This can dynamically adapt to the columns in the csv\n",
        "def apple_script_import2devonthink(docspath, csvpath_l, devonthink_database_name, db_folder_path):\n",
        "\tscript = f\"\"\"\n",
        "\t\t-- The path of the pdfs\n",
        "\t\tset DOCSPATH to \"{docspath}\"\n",
        "\n",
        "\t\t-- The path of the CSV local file after cleaning\n",
        "\t\tset CSVPATH_L to \"{csvpath_l}\"\n",
        "\n",
        "\t\t-- Database name\n",
        "\t\tset DBNAME to \"{devonthink_database_name}\"\n",
        "\t\t-- The destination of the exported Davonthink.\n",
        "\t\tset DB_FOLDER_PATH to \"{db_folder_path}\"\n",
        "\t\tset DBPATH to DB_FOLDER_PATH & \"/\" & DBNAME\n",
        "\n",
        "\n",
        "\t\ttell application id \"DNtp\"\n",
        "\n",
        "\t\t\t-- Create the new database\n",
        "\t\t\ttry\n",
        "\t\t\t\t-- Create the new database\n",
        "\t\t\t\tset newDb to create database POSIX file DBPATH as string\n",
        "\t\t\t\t-- set current database\n",
        "\t\t\t\tset curDb to current database\n",
        "\t\t\t\tlog \"Database created successfully at: \" & DBPATH\n",
        "\t\t\ton error errMsg number errorNumber\n",
        "\t\t\t\tdisplay dialog \"Failed to create database: \" & errMsg\n",
        "\t\t\t\treturn\n",
        "\t\t\tend try\n",
        "\n",
        "\n",
        "\t\t\t-- Import the cleaned csv\n",
        "\t\t\tset csv_id to import CSVPATH_L to current group\n",
        "\t\t\t-- Read the csv file from DevonThink (Because it's easier to handle within DavonThink.)\n",
        "\t\t\tset _csv_loc to location of csv_id\n",
        "\t\t\tset _csv_name to name of csv_id\n",
        "\t\t\tset _csv_loc_D to _csv_loc & _csv_name\n",
        "\t\t\tset csvFile to get record at (_csv_loc_D) in curDb\n",
        "\t\t\t-- Get the header names for the metadata names\n",
        "\t\t\tset csvHeaders to (columns of csvFile)\n",
        "\t\t\t-- Get the contents of the cells in the file\n",
        "\t\t\tset csvContents to (cells of csvFile)\n",
        "\n",
        "\n",
        "\t\t\t-- Traverse all the records in the CSV\n",
        "\t\t\trepeat with csvItem in csvContents\n",
        "\n",
        "\t\t\t\t-- Import the file, assuming the file path is in the first column\n",
        "\t\t\t\tset pdfFile to import (DOCSPATH & \"/\" & (item 1 of csvItem)) to current group\n",
        "\n",
        "\t\t\t\t-- Add the custom metadata dynamically based on the number of columns\n",
        "\t\t\t\t-- Item 1 is the file path, so we traverse from item 2\n",
        "\t\t        -- Minus one, because we add one useless column in the end\n",
        "\t\t\t\trepeat with i from 2 to ((count of csvHeaders) - 1)\n",
        "\t\t\t\t\tset mdKey to (item i of csvHeaders) as string\n",
        "\t\t\t\t\tset mdValue to (item i of csvItem)\n",
        "\t\t\t\t\tadd custom meta data mdValue for mdKey to pdfFile\n",
        "\t\t\t\tend repeat\n",
        "\n",
        "\t\t\tend repeat\n",
        "\n",
        "\t\t\t-- Delete that csv\n",
        "\t\t\tdelete record csv_id\t\t-- set theRecord to search \"csv_clean\"\n",
        "\n",
        "\t\tend tell\n",
        "\t\"\"\"\n",
        "\n",
        "\treturn script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "script = apple_script_import2devonthink(docspath=DOCSPATH, csvpath_l=CSVPATH_L, devonthink_database_name=DEVONTHINK_DATABASE_NAME, db_folder_path=DB_FOLDER_PATH)\n",
        "res = call_apple_script(script=script)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some tools for you to validate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check the number of files\n",
        "\n",
        "- May contain some multiple files in the file column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "if MODE == _FROM_BIBTEX_TO_DT:\n",
        "    DOCUMENT_FOLDER_PATH = UNZIP_BASE_PATH + \"/\" + \"files\"\n",
        "elif MODE == _FROM_ZOTERO_TO_DT:\n",
        "    DOCUMENT_FOLDER_PATH = CLEAN_DOCUMENT_FOLDER_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_files_in_folder(folder_path):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(\"Error: The specified path is not a directory.\")\n",
        "        return\n",
        "\n",
        "    file_count = 0\n",
        "    file_name = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        # Remove hidden items (Assume we won't set pdf or html as hidden files.)\n",
        "        files = [s for s in files if not s.startswith('.')]\n",
        "\n",
        "\n",
        "        file_count += len(files)\n",
        "        if len(files) > 0:\n",
        "            file_name += files\n",
        "\n",
        "\n",
        "    return file_count, file_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 641 files in the folder '/Users/tftuser/Desktop/Migrate/Exported Items_Pamala Haynes/files' and its subfolders.\n",
            "\n",
            "Here are the file lists\n",
            "1968-04-08InquiringPhotographer.pdf\n",
            "1968-05-04PhotoStandalone33.pdf\n",
            "1968-06-11haynesPhotoStandalone18.pdf\n",
            "1968-07-02haynesRubyDeeWelcomes.pdf\n",
            "1968-08-03haynesHowGetShove.pdf\n",
            "1968-08-17haynesNewLocalProduction.pdf\n",
            "1968-10-12haynesMovieProjectionistSeized.pdf\n",
            "1968-10-19PhotoStandaloneNo.pdf\n",
            "1969-01-07haynesBlackBookProgram.pdf\n",
            "1969-01-11haynesRubyDeeStar.pdf\n",
            "1969-02-11haynesBoxOfficeMagic.pdf\n",
            "1969-02-11haynesThisHistoricEra.pdf\n",
            "1969-03-18grayBlackLeadersRap.pdf\n",
            "1969-03-29grayCameraReview.pdf\n",
            "1969-04-08haynesPlayLampoonsMilitants.pdf\n",
            "1969-05-17haynesPhilaNAACPWill.pdf\n",
            "1969-05-24haynesManWhoCracked.pdf\n",
            "1969-06-17haynesTeenGangsTurn.pdf\n",
            "1969-06-24haynesPoitierSaysWhite.pdf\n",
            "1969-07-08wilderPhotoStandalone26.pdf\n",
            "1969-07-26PhotoStandaloneNo.pdf\n",
            "1969-09-06haynesTV1stBlackSponsored.pdf\n",
            "1969-09-13TVReviewStarts.pdf\n",
            "1969-09-16haynesJuliaHatersCalled.pdf\n",
            "1969-09-20haynesFeetDonFail.pdf\n",
            "1969-09-23haynesABCRoom222.pdf\n",
            "1969-09-25haynesJuliaHatersHypocrites.pdf\n",
            "1969-09-30haynesFlipWilsonBrash.pdf\n",
            "1969-09-30haynesFootballStarSimpson.pdf\n",
            "1969-10-04haynesTVGlorifiesViolence.pdf\n",
            "1969-10-07mcgeeJuliaSounds.pdf\n",
            "1969-10-09docyoungLetterEditorNo.pdf\n",
            "1969-10-18haynesSoulNewTV.pdf\n",
            "1969-10-25haynesEstablishmentBuyingMantua.pdf\n",
            "1969-10-25haynesOpenForum300.pdf\n",
            "1969-10-28haynesReporterAnalysisWhy.pdf\n",
            "1969-11-01haynesDickGregoryJoins.pdf\n",
            "1969-11-01haynesTVDramaPries.pdf\n",
            "1969-11-04haynesPreemptingFavoriteShows.pdf\n",
            "1969-11-18haynesSammyDavisTV.pdf\n",
            "1969-11-25haynesDickGregoryBlasts.pdf\n",
            "1969-11-25haynesIncensedReviewerSays.pdf\n",
            "1970 - She Disagrees-Letter to the Editor-Daily News 18 M.jpg\n",
            "1970 - Take Action-Letter to Editor-14 May 1970  Thu ·Pag.jpg\n",
            "1970 - TV listings Daily News 21 Nov 1970  Sat ·Page 14.jpg\n",
            "1970 - Worth of White Life Vs. Black Life-Pamala Haynes-D.jpg\n",
            "1970-01-06haynesOperationSoulHead.pdf\n",
            "1970-01-08haynesWriterClaimsTom.pdf\n",
            "1970-01-10haynesReviewerClaimsWatching.pdf\n",
            "1970-01-10haynesReviewerClaimsWatchinga.pdf\n",
            "1970-01-13haynesTVRoom222.pdf\n",
            "1970-01-24haynesMySweetCharlie.pdf\n",
            "1970-01-31haynesCarterArmyABC.pdf\n",
            "1970-02-03haynesJimBrownRole.pdf\n",
            "1970-02-03robinsonWeLoveYou.pdf\n",
            "1970-02-07haynesEvenActorBill.pdf\n",
            "1970-02-07haynesEvenActorBilla.pdf\n",
            "1970-02-14haynesStormSummerPowerful.pdf\n",
            "1970-02-21haynesInterracialTriangleHinted.pdf\n",
            "1970-02-21haynesKennedyDocumentaryKYWTV.pdf\n",
            "1970-02-21learShouldMarijuanaBe.pdf\n",
            "1970-02-24haynesDoctorLicenseWorked.pdf\n",
            "1970-03-21haynesLolaFalana1st.pdf\n",
            "1970-03-24haynesWinterSaysReal.pdf\n",
            "1970-03-24haynesWinterSaysReala.pdf\n",
            "1970-03-31haynesAndyJenkinsDenies.pdf\n",
            "1970-03-31haynesRight.pdf\n",
            "1970-03-31haynesWouldHaveMugged.pdf\n",
            "1970-04-07haynesRight.pdf\n",
            "1970-04-11haynesRight.pdf\n",
            "1970-04-11haynesRighta.pdf\n",
            "1970-04-11haynesWhiteRacismHas.pdf\n",
            "1970-04-14haynesRight.pdf\n",
            "1970-04-14haynesYorkCalledHeaven.pdf\n",
            "1970-04-18haynesPalatialWhiteProject.pdf\n",
            "1970-04-18haynesYorkBlacksClaim.pdf\n",
            "1970-04-21haynesRight.pdf\n",
            "1970-04-25haynesAnticsTVDr.pdf\n",
            "1970-04-25haynesRight.pdf\n",
            "1970-04-25haynesRighta.pdf\n",
            "1970-04-25haynesStockbroker27Sees.pdf\n",
            "1970-04-25TableContentsNo.pdf\n",
            "1970-04-28haynesRight.pdf\n",
            "1970-05-02haynesRight.pdf\n",
            "1970-05-02haynesShappWouldLet.pdf\n",
            "1970-05-05haynesCandidatePinsRacist.pdf\n",
            "1970-05-05haynesRighi.pdf\n",
            "1970-05-05haynesRighia.pdf\n",
            "1970-05-07WorthWhiteLife.pdf\n",
            "1970-05-09haynesRight.pdf\n",
            "1970-05-09haynesTV12Drama.pdf\n",
            "1970-05-09moselyNoAnswer.pdf\n",
            "1970-05-09TableContentsNo.pdf\n",
            "1970-05-12haynesRight.pdf\n",
            "1970-05-14TakeActionLetterEditor14.pdf\n",
            "1970-05-16haynesOakLaneWoman.pdf\n",
            "1970-05-16haynesRight.pdf\n",
            "1970-05-18SheDisagreesLetterEditorDaily.pdf\n",
            "1970-05-19haynesRight.pdf\n",
            "1970-05-19haynesRighta.pdf\n",
            "1970-05-23haynesRight.pdf\n",
            "1970-05-23haynesRighta.pdf\n",
            "1970-05-23haynesWarCrimesDrama.pdf\n",
            "1970-05-26haynesRight.pdf\n",
            "1970-05-30haynesRight.pdf\n",
            "1970-05-30haynesRighta.pdf\n",
            "1970-06-02haynesRight.pdf\n",
            "1970-06-02haynesRighta.pdf\n",
            "1970-06-02mooreReadersSayRebuttal.pdf\n",
            "1970-06-06haynesRight.pdf\n",
            "1970-06-06haynesRighta.pdf\n",
            "1970-06-09haynesRight.pdf\n",
            "1970-06-09haynesRighta.pdf\n",
            "1970-06-13haynesRighi.pdf\n",
            "1970-06-13haynesRighia.pdf\n",
            "1970-06-16haynesRight.pdf\n",
            "1970-06-16haynesRighta.pdf\n",
            "1970-06-20haynesLocalManFlip.pdf\n",
            "1970-06-20haynesRight.pdf\n",
            "1970-06-20haynesRighta.pdf\n",
            "1970-06-23haynesRight.pdf\n",
            "1970-06-30haynesRight.pdf\n",
            "1970-06-30haynesRighta.pdf\n",
            "1970-06-30PhotoStandalone15.pdf\n",
            "1970-07-07haynesRight.pdf\n",
            "1970-07-11haynesRight.pdf\n",
            "1970-07-14haynesRight.pdf\n",
            "1970-07-18haynesRight.pdf\n",
            "1970-07-21haynesRight.pdf\n",
            "1970-07-25haynesRight.pdf\n",
            "1970-07-28haynesRight.pdf\n",
            "1970-08-01haynesRight.pdf\n",
            "1970-08-01haynesRighta.pdf\n",
            "1970-08-04haynesRight.pdf\n",
            "1970-08-08haynesRight.pdf\n",
            "1970-08-08truthCommentsTribuneReaders.pdf\n",
            "1970-08-11haynesRight.pdf\n",
            "1970-08-15haynesRight.pdf\n",
            "1970-08-15haynesRighta.pdf\n",
            "1970-08-18haynesRight.pdf\n",
            "1970-08-22haynesRight.pdf\n",
            "1970-08-22haynesRighta.pdf\n",
            "1970-08-22haynesRightb.pdf\n",
            "1970-08-25haynesRight.pdf\n",
            "1970-08-29haynes200PoliceHelicopter.pdf\n",
            "1970-08-29haynesRight.pdf\n",
            "1970-09-01haynesRighi.pdf\n",
            "1970-09-05haynesRight.pdf\n",
            "1970-09-05haynesRighta.pdf\n",
            "1970-09-08haynesRight.pdf\n",
            "1970-09-12haynesRight.pdf\n",
            "1970-09-12haynesRighta.pdf\n",
            "1970-09-15haynesRight.pdf\n",
            "1970-09-19haynesRight.pdf\n",
            "1970-09-19haynesRighta.pdf\n",
            "1970-09-22haynesRight.pdf\n",
            "1970-09-26haynesRight.pdf\n",
            "1970-09-26haynesRighta.pdf\n",
            "1970-09-29haynesRight.pdf\n",
            "1970-10-03haynesRight.pdf\n",
            "1970-10-03haynesRighta.pdf\n",
            "1970-10-06haynesRight.pdf\n",
            "1970-10-10haynesRight.pdf\n",
            "1970-10-10haynesRighta.pdf\n",
            "1970-10-13haynesRight.pdf\n",
            "1970-10-17haynesRight.pdf\n",
            "1970-10-17haynesRighta.pdf\n",
            "1970-10-17haynesRightb.pdf\n",
            "1970-10-20haynesRight.pdf\n",
            "1970-10-24haynesRight.pdf\n",
            "1970-10-24haynesRighta.pdf\n",
            "1970-10-27haynesRight.pdf\n",
            "1970-10-31haynesRight.pdf\n",
            "1970-10-31haynesRighta.pdf\n",
            "1970-11-03haynesRight.pdf\n",
            "1970-11-07haynesRight.pdf\n",
            "1970-11-07haynesRighta.pdf\n",
            "1970-11-10haynesRight.pdf\n",
            "1970-11-14haynesRight.pdf\n",
            "1970-11-14haynesRighta.pdf\n",
            "1970-11-17haynesRight.pdf\n",
            "1970-11-21haynesRight.pdf\n",
            "1970-11-21haynesRighta.pdf\n",
            "1970-11-21TVListingsDaily.pdf\n",
            "1970-11-24haynesRight.pdf\n",
            "1970-11-28haynesRight.pdf\n",
            "1970-12-01haynesRight.pdf\n",
            "1970-12-05haynesRight.pdf\n",
            "1970-12-08haynesRight.pdf\n",
            "1970-12-12haynesWhitesCanHave.pdf\n",
            "1970-12-15haynesRight.pdf\n",
            "1970-12-19haynesRight.pdf\n",
            "1970-12-22haynesRight.pdf\n",
            "1970-12-26haynesRight.pdf\n",
            "1970-12-29haynesRight.pdf\n",
            "1971 - Arlen Specters Staff is Apologetic when They have.jpg\n",
            "1971 - Black Perspective on the News-The Philadelphia Inq.jpg\n",
            "1971 - Black Perspectives on the News-Philadelphia Daily .jpg\n",
            "1971 - Friday Evening April 9  1971-Black Perspectives on.jpg\n",
            "1971 - Pams Piece Makes Her Sick-22 Sep 1971  Wed ·Page .jpg\n",
            "1971 - Rizzo Called Creation of Phila Press-by Pamala H.jpg\n",
            "1971 - Writer calls Davis Super-Black Sam-Pamala Haynes.jpg\n",
            "1971-01-02haynesRight.pdf\n",
            "1971-01-05haynesRight.pdf\n",
            "1971-01-12haynesRight.pdf\n",
            "1971-01-16haynesRight.pdf\n",
            "1971-01-16haynesTewTvComedy.pdf\n",
            "1971-01-16haynesTewTvComedya.pdf\n",
            "1971-01-19haynesRight.pdf\n",
            "1971-01-23haynesRight.pdf\n",
            "1971-01-26haynesRight.pdf\n",
            "1971-01-26haynesSmithFamilyNew.pdf\n",
            "1971-01-28haynesNewTVComedy.pdf\n",
            "1971-01-30haynesRight.pdf\n",
            "1971-02-02haynesRight.pdf\n",
            "1971-02-06haynesRight.pdf\n",
            "1971-02-09haynesBlackInmatesAccuse.pdf\n",
            "1971-02-09haynesRight.pdf\n",
            "1971-02-13haynesRight.pdf\n",
            "1971-02-16haynesRight.pdf\n",
            "1971-02-20haynesRight.pdf\n",
            "1971-02-23haynesExInmateMastersHimself.pdf\n",
            "1971-02-27haynesRight.pdf\n",
            "1971-02-27haynesTrueLessonAmerican.pdf\n",
            "1971-03-02haynesRight.pdf\n",
            "1971-03-06haynesJohnKlmensLatest.pdf\n",
            "1971-03-06haynesJohnKlmensLatesta.pdf\n",
            "1971-03-06haynesRight.pdf\n",
            "1971-03-09haynesGraterfordWardenSued.pdf\n",
            "1971-03-09haynesRight.pdf\n",
            "1971-03-13haynesRight.pdf\n",
            "1971-03-16haynesRight.pdf\n",
            "1971-03-20haynesRight.pdf\n",
            "1971-03-23haynesRight.pdf\n",
            "1971-03-27haynesRight.pdf\n",
            "1971-03-30haynesRight.pdf\n",
            "1971-04-03haynesBlackPeopleUniversity.pdf\n",
            "1971-04-03haynesRight.pdf\n",
            "1971-04-04FridayEveningApril.pdf\n",
            "1971-04-06haynesRight.pdf\n",
            "1971-04-09BlackPerspectiveNews8PM.pdf\n",
            "1971-04-09BlackPerspectivesNewsPhiladelphia.pdf\n",
            "1971-04-10haynesRight.pdf\n",
            "1971-04-13EdHarrisReal.pdf\n",
            "1971-04-13haynesRight.pdf\n",
            "1971-04-17haynesRight.pdf\n",
            "1971-04-20haynesRight.pdf\n",
            "1971-04-24haynesRevAndersonSupport.pdf\n",
            "1971-04-24haynesRight.pdf\n",
            "1971-04-27haynesRight.pdf\n",
            "1971-04-27haynesRighta.pdf\n",
            "1971-05-01haynesRight.pdf\n",
            "1971-05-04haynesRight.pdf\n",
            "1971-05-08haynesRight.pdf\n",
            "1971-05-11haynesRight.pdf\n",
            "1971-05-11haynesRighta.pdf\n",
            "1971-05-15haynesRight.pdf\n",
            "1971-05-18haynesRight.pdf\n",
            "1971-05-22haynesRight.pdf\n",
            "1971-05-25haynesRight.pdf\n",
            "1971-05-29haynesRight.pdf\n",
            "1971-06-01haynesRight.pdf\n",
            "1971-06-05haynesRight.pdf\n",
            "1971-06-05haynesVandalsMessTV.pdf\n",
            "1971-06-08haynesRight.pdf\n",
            "1971-06-12haynesRight.pdf\n",
            "1971-06-15haynesRight.pdf\n",
            "1971-06-19haynesPortraitWarHero.pdf\n",
            "1971-06-19haynesRight.pdf\n",
            "1971-06-22haynesRight.pdf\n",
            "1971-06-26haynesRicht.pdf\n",
            "1971-06-29haynesRight.pdf\n",
            "1971-07-03haynesRight.pdf\n",
            "1971-07-06haynesPoliticsEducationPlay.pdf\n",
            "1971-07-06haynesRight.pdf\n",
            "1971-07-06haynesRighta.pdf\n",
            "1971-07-10haynesBlackExpoDenied.pdf\n",
            "1971-07-10haynesRIGHTIncompleteSource.pdf\n",
            "1971-07-13haynesRight.pdf\n",
            "1971-07-17haynesRight.pdf\n",
            "1971-07-20haynesRighi.pdf\n",
            "1971-07-24haynesRight.pdf\n",
            "1971-07-27haynesRight.pdf\n",
            "1971-07-31haynesRight.pdf\n",
            "1971-08-03haynesRight.pdf\n",
            "1971-08-07haynesBigBrotherNixon.pdf\n",
            "1971-08-07haynesRight.pdf\n",
            "1971-08-10haynesRight.pdf\n",
            "1971-08-14haynesRight.pdf\n",
            "1971-08-17haynesRighi.pdf\n",
            "1971-08-21haynesRight.pdf\n",
            "1971-08-24haynesRighi.pdf\n",
            "1971-08-31haynesRight.pdf\n",
            "1971-09-04haynesRight.pdf\n",
            "1971-09-07haynesRight.pdf\n",
            "1971-09-07haynesSEPTABusDriver.pdf\n",
            "1971-09-11haynesRighi.pdf\n",
            "1971-09-14haynesRight.pdf\n",
            "1971-09-17RizzoCalledCreation.pdf\n",
            "1971-09-21haynesRight.pdf\n",
            "1971-09-22PamPieceMakes.pdf\n",
            "1971-09-25haynesRight.pdf\n",
            "1971-09-28haynesRighi.pdf\n",
            "1971-10-02haynesRight.pdf\n",
            "1971-10-05haynesRight.pdf\n",
            "1971-10-09haynesRight.pdf\n",
            "1971-10-12haynesRight.pdf\n",
            "1971-10-16haynesRighi.pdf\n",
            "1971-10-19haynesRight.pdf\n",
            "1971-10-23haynesRight.pdf\n",
            "1971-10-23haynesRighta.pdf\n",
            "1971-10-26haynesRight.pdf\n",
            "1971-10-30haynesHowTimesUnfolded.pdf\n",
            "1971-10-30haynesRight.pdf\n",
            "1971-11-02haynesRight.pdf\n",
            "1971-11-06haynesBlackRealityUrged.pdf\n",
            "1971-11-06haynesRight.pdf\n",
            "1971-11-09haynesRight.pdf\n",
            "1971-11-13haynesRight.pdf\n",
            "1971-11-16haynesRight.pdf\n",
            "1971-11-20haynesRIGHT.pdf\n",
            "1971-11-23haynesRight.pdf\n",
            "1971-11-27haynesRight.pdf\n",
            "1971-11-27haynesRIGHTPhillyDoublecross.pdf\n",
            "1971-12-04haynesBrianSongRates.pdf\n",
            "1971-12-04haynesRight.pdf\n",
            "1971-12-04haynesRIGHTBlacksAngry.pdf\n",
            "1971-12-07haynesRight.pdf\n",
            "1971-12-09ArlenSpecterStaff.pdf\n",
            "1971-12-11haynesRighi.pdf\n",
            "1971-12-11haynesRIGHTRaceClass.pdf\n",
            "1971-12-14haynesRight.pdf\n",
            "1971-12-18haynesRight.pdf\n",
            "1971-12-18haynesRIGHTCaseCrime.pdf\n",
            "1971-12-21haynesRight.pdf\n",
            "1971-12-25haynesRight.pdf\n",
            "1971-12-25haynesRighta.pdf\n",
            "1971-12-25haynesRIGHTGallPoliticians.pdf\n",
            "1971-12-28haynesRight.pdf\n",
            "1971-12-30WriterCallsDavis.pdf\n",
            "1972 - Letters to the Editor on Pamala Haynes-Daily News.jpg\n",
            "1972 - Miss Haynes Will Preside at Awards-Philadelphia In.jpg\n",
            "1972 - Off Target-Letter to the Editor-Phila Daily News-0.jpg\n",
            "1972 - She Stinks-Letter to the Editor-Philadelphia Daily.jpg\n",
            "1972-01-04haynesRight.pdf\n",
            "1972-01-08haynesRight.pdf\n",
            "1972-01-08haynesRighta.pdf\n",
            "1972-01-08TargetLetterEditorPhilaDaily.pdf\n",
            "1972-01-10SheStinksLetterEditorPhiladelphia.pdf\n",
            "1972-01-11haynesRight.pdf\n",
            "1972-01-15haynesRight.pdf\n",
            "1972-01-15haynesRIGHTSammyDavis.pdf\n",
            "1972-01-18haynesRight.pdf\n",
            "1972-01-22haynesRight.pdf\n",
            "1972-01-25haynesRight.pdf\n",
            "1972-01-25LettersEditorPamala.pdf\n",
            "1972-01-29haynesRight.pdf\n",
            "1972-02-01haynesRight.pdf\n",
            "1972-02-05haynesFacesGenocide.pdf\n",
            "1972-02-05haynesRight.pdf\n",
            "1972-02-08haynesRighi.pdf\n",
            "1972-02-12haynesNoRoomRevolution.pdf\n",
            "1972-02-12haynesRight.pdf\n",
            "1972-02-15haynesRighi.pdf\n",
            "1972-02-19haynesRight.pdf\n",
            "1972-02-22haynesRight.pdf\n",
            "1972-02-24haynesTwoFacesGenocide.pdf\n",
            "1972-02-26haynesRight.pdf\n",
            "1972-02-26haynesRIGHTBlacksPolitics.pdf\n",
            "1972-02-29haynesRight.pdf\n",
            "1972-03-04haynesComingSecondReconstruction.pdf\n",
            "1972-03-04haynesRight.pdf\n",
            "1972-03-07haynesRight.pdf\n",
            "1972-03-11haynesRighi.pdf\n",
            "1972-03-14haynesRight.pdf\n",
            "1972-03-18haynesRight.pdf\n",
            "1972-03-18haynesRIGHTChopSuey.pdf\n",
            "1972-03-21haynesRight.pdf\n",
            "1972-03-25haynesRight.pdf\n",
            "1972-03-28haynesRight.pdf\n",
            "1972-04-01haynesRight.pdf\n",
            "1972-04-01haynesRIGHTONGaryConvention.pdf\n",
            "1972-04-04haynesRight.pdf\n",
            "1972-04-04levinCommentsTribuneRcaders.pdf\n",
            "1972-04-08haynesRIGHTPoliticosWage.pdf\n",
            "1972-04-11haynesRight.pdf\n",
            "1972-04-15haynesRight.pdf\n",
            "1972-04-15haynesRighta.pdf\n",
            "1972-04-15haynesRizzoApparentSupport.pdf\n",
            "1972-04-18haynesRight.pdf\n",
            "1972-04-22haynesRight.pdf\n",
            "1972-04-22haynesRighta.pdf\n",
            "1972-04-25haynesRight.pdf\n",
            "1972-04-29haynesRight.pdf\n",
            "1972-04-29haynesRighta.pdf\n",
            "1972-05-02haynesRight.pdf\n",
            "1972-05-06haynesRight.pdf\n",
            "1972-05-06haynesRighta.pdf\n",
            "1972-05-13haynesRight.pdf\n",
            "1972-05-13haynesRighton.pdf\n",
            "1972-05-16haynesRight.pdf\n",
            "1972-05-16WestPhilaAlumni.pdf\n",
            "1972-05-20haynesRight.pdf\n",
            "1972-05-20haynesRighta.pdf\n",
            "1972-05-23haynesRight.pdf\n",
            "1972-05-27haynesRight.pdf\n",
            "1972-05-27haynesRighta.pdf\n",
            "1972-05-30haynesRight.pdf\n",
            "1972-05-31MissHaynesWill.pdf\n",
            "1972-06-03haynesRight.pdf\n",
            "1972-06-03haynesRIGHTSenatorHankins.pdf\n",
            "1972-06-06haynesRight.pdf\n",
            "1972-06-10haynesRight.pdf\n",
            "1972-06-10haynesRighta.pdf\n",
            "1972-06-13haynesRIGHTHarrisburgHotline.pdf\n",
            "1972-06-17haynesRight.pdf\n",
            "1972-06-17haynesRighton.pdf\n",
            "1972-06-20haynesRight.pdf\n",
            "1972-06-24haynesRight.pdf\n",
            "1972-06-24haynesRighta.pdf\n",
            "1972-06-27haynesRight.pdf\n",
            "1972-07-01haynesRight.pdf\n",
            "1972-07-01haynesRighta.pdf\n",
            "1972-07-04haynesRight.pdf\n",
            "1972-07-08haynesRight.pdf\n",
            "1972-07-08haynesRignt.pdf\n",
            "1972-07-08haynesRignta.pdf\n",
            "1972-07-15haynesRight.pdf\n",
            "1972-07-15haynesRighta.pdf\n",
            "1972-07-22haynesRight.pdf\n",
            "1972-07-25haynesRight.pdf\n",
            "1972-07-29haynesRight.pdf\n",
            "1972-08-01haynesRight.pdf\n",
            "1972-08-05haynesRight.pdf\n",
            "1972-08-05haynesRighta.pdf\n",
            "1972-08-08haynesRight.pdf\n",
            "1972-08-12haynesRight.pdf\n",
            "1972-08-12haynesRighta.pdf\n",
            "1972-08-15haynesRight.pdf\n",
            "1972-08-19haynesRight.pdf\n",
            "1972-08-19haynesRighta.pdf\n",
            "1972-08-22haynesRight.pdf\n",
            "1972-08-26haynesRight.pdf\n",
            "1972-08-26RIGHIPAMALAHAYNES.pdf\n",
            "1972-08-29haynesRight.pdf\n",
            "1972-09-02haynesRight.pdf\n",
            "1972-09-02haynesRighta.pdf\n",
            "1972-09-02haynesRightb.pdf\n",
            "1972-09-05haynesRight.pdf\n",
            "1972-09-09haynesRight.pdf\n",
            "1972-09-09haynesRighta.pdf\n",
            "1972-09-12haynesRighi.pdf\n",
            "1972-09-12rogersOrchidsPamala.pdf\n",
            "1972-09-16haynesRight.pdf\n",
            "1972-09-16haynesRighta.pdf\n",
            "1972-09-19haynesRight.pdf\n",
            "1972-09-23haynesRight.pdf\n",
            "1972-09-23haynesRighta.pdf\n",
            "1972-09-26haynesRight.pdf\n",
            "1972-09-30haynesRight.pdf\n",
            "1972-09-30haynesRighta.pdf\n",
            "1972-10-03haynesRight.pdf\n",
            "1972-10-07haynesRight.pdf\n",
            "1972-10-07haynesRighta.pdf\n",
            "1972-10-10haynesRight.pdf\n",
            "1972-10-14haynesRight.pdf\n",
            "1972-10-14haynesRighton.pdf\n",
            "1972-10-17haynesRight.pdf\n",
            "1972-10-19PresidentialRaceMcGovern.pdf\n",
            "1972-10-21haynesRight.pdf\n",
            "1972-10-24haynesRighi.pdf\n",
            "1972-10-28haynesRighii.pdf\n",
            "1972-10-28haynesRight.pdf\n",
            "1972-10-31haynesRight.pdf\n",
            "1972-11-04haynesRight.pdf\n",
            "1972-11-07haynesRight.pdf\n",
            "1972-11-11haynesRight.pdf\n",
            "1972-11-11haynesRighta.pdf\n",
            "1972-11-14haynesRight.pdf\n",
            "1972-11-18haynesRighi.pdf\n",
            "1972-11-18haynesRight.pdf\n",
            "1972-11-21haynesRight.pdf\n",
            "1972-11-25haynesRight.pdf\n",
            "1972-11-28haynesRighi.pdf\n",
            "1972-12-02haynesRight.pdf\n",
            "1972-12-05haynesRight.pdf\n",
            "1972-12-09haynesRight.pdf\n",
            "1972-12-09haynesRighta.pdf\n",
            "1972-12-12haynesRight.pdf\n",
            "1972-12-16haynesRight.pdf\n",
            "1972-12-19haynesRight.pdf\n",
            "1972-12-23haynesRight.pdf\n",
            "1972-12-23haynesRighta.pdf\n",
            "1972-12-26haynesRight.pdf\n",
            "1972-12-30haynesRighi.pdf\n",
            "1972-12-30haynesRight.pdf\n",
            "1972haynesRight.pdf\n",
            "1972haynesRighta.pdf\n",
            "1973 - 2 on Bulletin Reassigned After TV Comments on Rizz.jpg\n",
            "1973 - Harry Harris-Rizzo Hailed  Panned on National TV S.jpg\n",
            "1973 - Harry Harris-Screening TV-Black Perspective Blazes.jpg\n",
            "1973 - Pamala a 2nd Taylor Grand--Chuck Stone--Daily News.jpg\n",
            "1973 - TV for Monday-The Philadelphia Inquirer 20 Aug 197.jpg\n",
            "1973 - TV Listings-April 13  1973  8 p44.jpg\n",
            "1973-01-02haynesRighi.pdf\n",
            "1973-01-06haynesRight.pdf\n",
            "1973-01-06haynesRighta.pdf\n",
            "1973-01-06haynesRightb.pdf\n",
            "1973-01-08HarryHarrisRizzoHailed.pdf\n",
            "1973-01-09haynesRight.pdf\n",
            "1973-01-12BulletinReassignedTV.pdf\n",
            "1973-01-13haynesRight.pdf\n",
            "1973-01-13haynesRighta.pdf\n",
            "1973-01-16haynesRighi.pdf\n",
            "1973-01-20haynesRight.pdf\n",
            "1973-01-20haynesRighta.pdf\n",
            "1973-01-23haynesRight.pdf\n",
            "1973-01-27haynesNBCAmericaShow.pdf\n",
            "1973-01-27haynesRight.pdf\n",
            "1973-01-27haynesRighta.pdf\n",
            "1973-01-30haynesRigiit.pdf\n",
            "1973-02-03haynesRight.pdf\n",
            "1973-02-03haynesRighta.pdf\n",
            "1973-02-06haynesRight.pdf\n",
            "1973-02-10haynesRight.pdf\n",
            "1973-02-10haynesRighta.pdf\n",
            "1973-02-10haynesRightb.pdf\n",
            "1973-02-13haynesRighi.pdf\n",
            "1973-02-17haynesRight.pdf\n",
            "1973-02-17haynesRighta.pdf\n",
            "1973-02-17haynesRightb.pdf\n",
            "1973-02-17haynesRightc.pdf\n",
            "1973-02-20haynesRight.pdf\n",
            "1973-02-24haynesRight.pdf\n",
            "1973-02-24haynesRighta.pdf\n",
            "1973-02-27haynesRight.pdf\n",
            "1973-03-03haynesRight.pdf\n",
            "1973-03-03haynesRighta.pdf\n",
            "1973-03-06haynesRight.pdf\n",
            "1973-03-06haynesRighta.pdf\n",
            "1973-03-10haynesRight.pdf\n",
            "1973-03-10haynesRighta.pdf\n",
            "1973-03-13haynesRight.pdf\n",
            "1973-03-13haynesRighta.pdf\n",
            "1973-03-17haynesRight.pdf\n",
            "1973-03-17haynesRighta.pdf\n",
            "1973-03-24haynesRight.pdf\n",
            "1973-03-27haynesRighit.pdf\n",
            "1973-03-31haynesRight.pdf\n",
            "1973-03-31haynesRighta.pdf\n",
            "1973-04-07haynesRight.pdf\n",
            "1973-04-07haynesRighta.pdf\n",
            "1973-04-10haynesRight.pdf\n",
            "1973-04-14haynesRight.pdf\n",
            "1973-04-14haynesRIGHTVDEducation.pdf\n",
            "1973-04-17haynesRight.pdf\n",
            "1973-04-21haynesRighi.pdf\n",
            "1973-04-21haynesRight.pdf\n",
            "1973-04-24haynesRight.pdf\n",
            "1973-04-25HarryHarrisScreeningTVBlack.pdf\n",
            "1973-04-28haynesRight.pdf\n",
            "1973-04-28haynesRighta.pdf\n",
            "1973-05-01haynesRight.pdf\n",
            "1973-05-05haynesRight.pdf\n",
            "1973-05-05haynesRighta.pdf\n",
            "1973-05-12haynesRight.pdf\n",
            "1973-05-12haynesRighta.pdf\n",
            "1973-05-15haynesWatergateAnalysis.pdf\n",
            "1973-05-19haynesRight.pdf\n",
            "1973-05-26haynesRight.pdf\n",
            "1973-06-02haynesRight.pdf\n",
            "1973-07-14PastorLynnhavenHonored.pdf\n",
            "1973-07-27Pamala2ndTaylor.pdf\n",
            "1973-07-28haynesRepBarbaraJordan.pdf\n",
            "1973-08-11haynesAgnewJekyllHydeMaryland.pdf\n",
            "1973-08-12TVListingsApril13.pdf\n",
            "1973-08-14haynesWatergateLocalCop.pdf\n",
            "1973-08-18haynesRizzoNixonTry.pdf\n",
            "1973-08-20TVMondayThePhiladelphia.pdf\n",
            "1973-08-21haynesIndictmentAgnewSays.pdf\n",
            "1973-08-25haynesNixonPressConference.pdf\n",
            "1973-08-28haynesWasNixonDeath.pdf\n",
            "1973-09-01haynesAnalysisNixonMaking.pdf\n",
            "1973-09-04haynesAnalysisTimeBribing.pdf\n",
            "1973-09-11haynesTVBeerNibbles.pdf\n",
            "1973-09-15haynesTelevisionWouldYou.pdf\n",
            "1973-09-22haynesChamberlainSlursBlack.pdf\n",
            "1973-10-02haynesBLACKWOMANREVIEWS.pdf\n",
            "1973-10-13haynesAnalysisWhatHappened.pdf\n",
            "1973-10-16haynesAnalysisLyndonJohnson.pdf\n",
            "1973-10-20haynesANALYSISAtlantaGets.pdf\n",
            "1973-11-03haynesANALYSISNixonTape.pdf\n",
            "1973-11-10haynesMuhammadAliJoe.pdf\n",
            "1973-12-08haynesWillSpecterJump.pdf\n",
            "1973-12-15haynesSalaryGrabPasses.pdf\n",
            "1974 - But the Big Fight is Between Hochman and Haynes-Ph.jpg\n",
            "1974 - Event--Analysis of Fords first 100 days-22 Nov 19.jpg\n",
            "1974 - Jim OBrien on TV-Daily News-06 Dec 1974  Fri ·Pag.jpg\n",
            "1974 - Woman Replies to Big Lies about Women-Pamala Hayne.jpg\n",
            "1974-01-26haynesRichardsonDilworthPolitician.pdf\n",
            "1974-01-28WomanRepliesBig.pdf\n",
            "1974-02-05TvHighlights.pdf\n",
            "1974-03-12haynesReportSupportsBrutality.pdf\n",
            "1974-05-14haynesAreaJewishSpokesman.pdf\n",
            "1974-06-04haynesBlackFiremenHit.pdf\n",
            "1974-06-04haynesBlackFiremenHita.pdf\n",
            "1974-06-04OtherNoTitle.pdf\n",
            "1974-06-18haynesBlackHeroWatergate.pdf\n",
            "1974-06-18haynesBlackHeroWatergatea.pdf\n",
            "1974-06-22haynesWardLeaderMrs.pdf\n",
            "1974-07-13haynesUnbornChildrenWelfare.pdf\n",
            "1974-08-03haynesPresidentNixonHas.pdf\n",
            "1974-08-13haynesWillGeraldFord.pdf\n",
            "1974-09-10haynesPresidentFordGives.pdf\n",
            "1974-09-14haynesAngryPublicForces.pdf\n",
            "1974-09-17haynesFredWilliamsonFired.pdf\n",
            "1974-09-17haynesFredWilliamsonFireda.pdf\n",
            "1974-09-23BigFightHochman.pdf\n",
            "1974-10-05haynesRacismMayBe.pdf\n",
            "1974-10-08haynesFordReadyKO.pdf\n",
            "1974-10-19haynesPoliticiansGetBrickbats.pdf\n",
            "1974-11-02BlackJournalistsForm.pdf\n",
            "1974-11-02haynesGladysBondHopes.pdf\n",
            "1974-11-09haynesBlackVotersContinue.pdf\n",
            "1974-11-22EventAnalysisFord.pdf\n",
            "1974-12-06JimBrienTVDaily.pdf\n",
            "1976 - Channel 3 Jostling Ancors--Harry Harris.jpg\n",
            "1976 - Haynes Gets Andreas Job-Daily News 23 Nov 1976  T.jpg\n",
            "1976 - The Scene The Philadelphia Inquirer 03 Dec 1976  F.jpg\n",
            "1976-11-23HaynesGetsAndrea.pdf\n",
            "1976-11-29ChannelJostlingAncors.pdf\n",
            "1976-12-03ScenePhiladelphiaInquirer.pdf\n",
            "1977 - 1-Tribunes Tribulations--Jerome Mondesire-Philade.jpg\n",
            "1977-11-131TribuneTribulationsJerome.pdf\n",
            "1978 - Our Mayor--Daily News 17 Mar 1978  Fri ·Page 22.jpg\n",
            "1978 - Rizzos White Rights Crusade  Daily News  March 17.jpg\n",
            "1978-03-17OurMayorDaily.pdf\n",
            "1978-03-17RizzoWhiteRights.pdf\n",
            "1980 - Chuck Stone-Women Incommunicado  Inc-Daily News 01.jpg\n",
            "1980-05-01ChuckStoneWomenIncommunicado.pdf\n",
            "out.jpg\n"
          ]
        }
      ],
      "source": [
        "num_files, name_files = count_files_in_folder(folder_path=DOCUMENT_FOLDER_PATH)\n",
        "print(f\"There are {num_files} files in the folder '{DOCUMENT_FOLDER_PATH}' and its subfolders.\")\n",
        "print()\n",
        "\n",
        "print(\"Here are the file lists\")\n",
        "name_files.sort(key=lambda x: x.lower())\n",
        "for i_file in name_files:\n",
        "    print(i_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some useful excel functions\n",
        "\n",
        "1. Get the file name from the file path\n",
        "\n",
        "```excel\n",
        "=MID(A51, FIND(\"@\", SUBSTITUTE(A51, \"/\", \"@\", LEN(A51)-LEN(SUBSTITUTE(A51, \"/\", \"\"))))+1, LEN(A51))\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
